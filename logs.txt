
==> Audit <==
|--------------|---------------------|----------|------|---------|---------------------|---------------------|
|   Command    |        Args         | Profile  | User | Version |     Start Time      |      End Time       |
|--------------|---------------------|----------|------|---------|---------------------|---------------------|
| start        |                     | minikube | berk | v1.33.1 | 21 Jun 24 09:47 +03 |                     |
| start        |                     | minikube | berk | v1.33.1 | 21 Jun 24 09:47 +03 | 21 Jun 24 09:50 +03 |
| dashboard    |                     | minikube | berk | v1.33.1 | 21 Jun 24 09:51 +03 |                     |
| delete       | --all               | minikube | berk | v1.33.1 | 21 Jun 24 09:54 +03 | 21 Jun 24 09:54 +03 |
| start        |                     | minikube | berk | v1.33.1 | 21 Jun 24 10:10 +03 | 21 Jun 24 10:11 +03 |
| dashboard    |                     | minikube | berk | v1.33.1 | 21 Jun 24 10:23 +03 |                     |
| service      | first-app           | minikube | berk | v1.33.1 | 21 Jun 24 11:00 +03 | 21 Jun 24 11:09 +03 |
| service      | first-app           | minikube | berk | v1.33.1 | 21 Jun 24 11:10 +03 | 21 Jun 24 11:12 +03 |
| dashboard    |                     | minikube | berk | v1.33.1 | 21 Jun 24 11:12 +03 |                     |
| service      | first-app           | minikube | berk | v1.33.1 | 21 Jun 24 11:28 +03 | 21 Jun 24 11:31 +03 |
| service      | first-app           | minikube | berk | v1.33.1 | 21 Jun 24 11:31 +03 | 21 Jun 24 11:37 +03 |
| service      | first-app           | minikube | berk | v1.33.1 | 21 Jun 24 11:37 +03 | 21 Jun 24 11:39 +03 |
| service      | first-app           | minikube | berk | v1.33.1 | 21 Jun 24 11:39 +03 | 21 Jun 24 11:40 +03 |
| update-check |                     | minikube | berk | v1.33.1 | 21 Jun 24 11:57 +03 | 21 Jun 24 11:57 +03 |
| update-check |                     | minikube | berk | v1.33.1 | 21 Jun 24 11:58 +03 | 21 Jun 24 11:58 +03 |
| service      | backend             | minikube | berk | v1.33.1 | 21 Jun 24 12:32 +03 | 21 Jun 24 12:35 +03 |
| service      | backend             | minikube | berk | v1.33.1 | 21 Jun 24 12:35 +03 | 21 Jun 24 12:35 +03 |
| service      | backend             | minikube | berk | v1.33.1 | 21 Jun 24 12:36 +03 | 21 Jun 24 12:36 +03 |
| service      | backend             | minikube | berk | v1.33.1 | 21 Jun 24 12:36 +03 | 21 Jun 24 12:41 +03 |
| update-check |                     | minikube | berk | v1.33.1 | 21 Jun 24 14:27 +03 | 21 Jun 24 14:27 +03 |
| start        |                     | minikube | berk | v1.33.1 | 21 Jun 24 14:30 +03 | 21 Jun 24 14:31 +03 |
| service      | backend             | minikube | berk | v1.33.1 | 21 Jun 24 14:31 +03 | 21 Jun 24 14:31 +03 |
| update-check |                     | minikube | berk | v1.33.1 | 21 Jun 24 14:47 +03 | 21 Jun 24 14:47 +03 |
| update-check |                     | minikube | berk | v1.33.1 | 21 Jun 24 15:09 +03 | 21 Jun 24 15:09 +03 |
| service      | story-service       | minikube | berk | v1.33.1 | 21 Jun 24 15:30 +03 | 21 Jun 24 15:38 +03 |
| service      | story-service       | minikube | berk | v1.33.1 | 21 Jun 24 15:45 +03 |                     |
| update-check |                     | minikube | berk | v1.33.1 | 21 Jun 24 21:15 +03 | 21 Jun 24 21:15 +03 |
| update-check |                     | minikube | berk | v1.33.1 | 21 Jun 24 21:45 +03 | 21 Jun 24 21:45 +03 |
| update-check |                     | minikube | berk | v1.33.1 | 21 Jun 24 21:46 +03 | 21 Jun 24 21:46 +03 |
| service      | go-rest-api-service | minikube | berk | v1.33.1 | 21 Jun 24 22:08 +03 |                     |
| delete       | --all               | minikube | berk | v1.33.1 | 21 Jun 24 22:08 +03 | 21 Jun 24 22:08 +03 |
| start        |                     | minikube | berk | v1.33.1 | 21 Jun 24 22:09 +03 | 21 Jun 24 22:09 +03 |
| service      | go-rest-api-service | minikube | berk | v1.33.1 | 21 Jun 24 22:10 +03 |                     |
| service      | go-rest-api-service | minikube | berk | v1.33.1 | 21 Jun 24 22:11 +03 |                     |
|--------------|---------------------|----------|------|---------|---------------------|---------------------|


==> Last Start <==
Log file created at: 2024/06/21 22:09:00
Running on machine: DESKTOP-2C4S64N
Binary: Built with gc go1.22.1 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0621 22:09:00.872433  231799 out.go:291] Setting OutFile to fd 1 ...
I0621 22:09:00.872577  231799 out.go:343] isatty.IsTerminal(1) = true
I0621 22:09:00.872580  231799 out.go:304] Setting ErrFile to fd 2...
I0621 22:09:00.872584  231799 out.go:343] isatty.IsTerminal(2) = true
I0621 22:09:00.872727  231799 root.go:338] Updating PATH: /home/berk/.minikube/bin
I0621 22:09:00.873432  231799 out.go:298] Setting JSON to false
I0621 22:09:00.875698  231799 start.go:129] hostinfo: {"hostname":"DESKTOP-2C4S64N","uptime":27790,"bootTime":1718969150,"procs":86,"os":"linux","platform":"ubuntu","platformFamily":"debian","platformVersion":"22.04","kernelVersion":"5.15.153.1-microsoft-standard-WSL2","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"guest","hostId":"1addef89-b27b-495c-86e5-a2bf56baa36f"}
I0621 22:09:00.875756  231799 start.go:139] virtualization:  guest
I0621 22:09:00.878498  231799 out.go:177] üòÑ  minikube v1.33.1 on Ubuntu 22.04 (amd64)
I0621 22:09:00.882633  231799 notify.go:220] Checking for updates...
I0621 22:09:00.883692  231799 driver.go:392] Setting default libvirt URI to qemu:///system
I0621 22:09:00.883967  231799 global.go:112] Querying for installed drivers using PATH=/home/berk/.minikube/bin:/home/berk/.pyenv/plugins/pyenv-virtualenv/shims:/home/berk/.pyenv/shims:/home/berk/.vscode-server/bin/5437499feb04f7a586f677b155b039bc2b3669eb/bin/remote-cli:/home/berk/.local/bin:/home/berk/.nvm/versions/node/v20.14.0/bin:/home/berk/.local/bin:/home/berk/.pyenv/bin:/home/berk/.pyenv/plugins/pyenv-virtualenv/shims:/home/berk/.pyenv/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/usr/lib/wsl/lib:/mnt/c/Windows/system32:/mnt/c/Windows:/mnt/c/Windows/System32/Wbem:/mnt/c/Windows/System32/WindowsPowerShell/v1.0/:/mnt/c/Windows/System32/OpenSSH/:/mnt/c/Program Files (x86)/NVIDIA Corporation/PhysX/Common:/mnt/c/Program Files/NVIDIA Corporation/NVIDIA NvDLISR:/Docker/host/bin:/mnt/c/Users/BERK/AppData/Local/Programs/Python/Python312/Scripts/:/mnt/c/Users/BERK/AppData/Local/Programs/Python/Python312/:/mnt/c/Users/BERK/AppData/Local/Microsoft/WindowsApps:/mnt/c/Users/BERK/AppData/Local/Programs/Microsoft VS Code/bin:/snap/bin:/home/berk/go/bin:/usr/local/go/bin:/home/berk/go/bin
I0621 22:09:00.884030  231799 global.go:133] ssh default: false priority: 4, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0621 22:09:00.905555  231799 global.go:133] kvm2 default: true priority: 8, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "virsh": executable file not found in $PATH Reason: Fix:Install libvirt Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/kvm2/ Version:}
I0621 22:09:00.930572  231799 global.go:133] qemu2 default: true priority: 7, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "qemu-system-x86_64": executable file not found in $PATH Reason: Fix:Install qemu-system Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/qemu/ Version:}
I0621 22:09:00.965090  231799 global.go:133] virtualbox default: true priority: 6, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:unable to find VBoxManage in $PATH Reason: Fix:Install VirtualBox Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/virtualbox/ Version:}
I0621 22:09:00.981316  231799 global.go:133] vmware default: false priority: 5, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "vmrun": executable file not found in $PATH Reason: Fix:Install vmrun Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/vmware/ Version:}
I0621 22:09:01.027427  231799 docker.go:122] docker version: linux-26.1.4:Docker Desktop
I0621 22:09:01.027657  231799 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0621 22:09:01.174973  231799 info.go:266] docker info: {ID:cf3eecfe-f5ab-4e6e-bf37-34939c173ba0 Containers:0 ContainersRunning:0 ContainersPaused:0 ContainersStopped:0 Images:11 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:53 OomKillDisable:true NGoroutines:76 SystemTime:2024-06-21 19:09:01.160314766 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:11 KernelVersion:5.15.153.1-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:8269107200 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=unix:///var/run/docker-cli.sock] ExperimentalBuild:false ServerVersion:26.1.4 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:d2d58213f83a351ca8f528a95fbd145f5654e957 Expected:d2d58213f83a351ca8f528a95fbd145f5654e957} RuncCommit:{ID:v1.1.12-0-g51d5e94 Expected:v1.1.12-0-g51d5e94} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/local/lib/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.14.1-desktop.1] map[Name:compose Path:/usr/local/lib/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.27.1-desktop.1] map[Name:debug Path:/usr/local/lib/docker/cli-plugins/docker-debug SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.32] map[Name:dev Path:/usr/local/lib/docker/cli-plugins/docker-dev SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:/usr/local/lib/docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.24] map[Name:feedback Path:/usr/local/lib/docker/cli-plugins/docker-feedback SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.5] map[Name:init Path:/usr/local/lib/docker/cli-plugins/docker-init SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.2.0] map[Name:sbom Path:/usr/local/lib/docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:/usr/local/lib/docker/cli-plugins/docker-scout SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.9.3]] Warnings:<nil>}}
I0621 22:09:01.175115  231799 docker.go:295] overlay module found
I0621 22:09:01.175152  231799 global.go:133] docker default: true priority: 9, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0621 22:09:01.197919  231799 global.go:133] none default: false priority: 4, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:running the 'none' driver as a regular user requires sudo permissions Reason: Fix: Doc: Version:}
I0621 22:09:01.214256  231799 global.go:133] podman default: true priority: 7, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "podman": executable file not found in $PATH Reason: Fix:Install Podman Doc:https://minikube.sigs.k8s.io/docs/drivers/podman/ Version:}
I0621 22:09:01.214438  231799 driver.go:314] not recommending "ssh" due to default: false
I0621 22:09:01.214452  231799 driver.go:349] Picked: docker
I0621 22:09:01.214458  231799 driver.go:350] Alternatives: [ssh]
I0621 22:09:01.214463  231799 driver.go:351] Rejects: [kvm2 qemu2 virtualbox vmware none podman]
I0621 22:09:01.215997  231799 out.go:177] ‚ú®  Automatically selected the docker driver
I0621 22:09:01.217571  231799 start.go:297] selected driver: docker
I0621 22:09:01.217598  231799 start.go:901] validating driver "docker" against <nil>
I0621 22:09:01.217610  231799 start.go:912] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0621 22:09:01.217920  231799 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0621 22:09:01.320462  231799 info.go:266] docker info: {ID:cf3eecfe-f5ab-4e6e-bf37-34939c173ba0 Containers:0 ContainersRunning:0 ContainersPaused:0 ContainersStopped:0 Images:11 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:53 OomKillDisable:true NGoroutines:76 SystemTime:2024-06-21 19:09:01.307496895 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:11 KernelVersion:5.15.153.1-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:8269107200 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=unix:///var/run/docker-cli.sock] ExperimentalBuild:false ServerVersion:26.1.4 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:d2d58213f83a351ca8f528a95fbd145f5654e957 Expected:d2d58213f83a351ca8f528a95fbd145f5654e957} RuncCommit:{ID:v1.1.12-0-g51d5e94 Expected:v1.1.12-0-g51d5e94} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/local/lib/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.14.1-desktop.1] map[Name:compose Path:/usr/local/lib/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.27.1-desktop.1] map[Name:debug Path:/usr/local/lib/docker/cli-plugins/docker-debug SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.32] map[Name:dev Path:/usr/local/lib/docker/cli-plugins/docker-dev SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:/usr/local/lib/docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.24] map[Name:feedback Path:/usr/local/lib/docker/cli-plugins/docker-feedback SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.5] map[Name:init Path:/usr/local/lib/docker/cli-plugins/docker-init SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.2.0] map[Name:sbom Path:/usr/local/lib/docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:/usr/local/lib/docker/cli-plugins/docker-scout SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.9.3]] Warnings:<nil>}}
I0621 22:09:01.320774  231799 start_flags.go:310] no existing cluster config was found, will generate one from the flags 
I0621 22:09:01.321805  231799 start_flags.go:393] Using suggested 2200MB memory alloc based on sys=7886MB, container=7886MB
I0621 22:09:01.322606  231799 start_flags.go:929] Wait components to verify : map[apiserver:true system_pods:true]
I0621 22:09:01.324186  231799 out.go:177] üìå  Using Docker driver with root privileges
W0621 22:09:01.325794  231799 out.go:239] ‚ùó  For an improved experience it's recommended to use Docker Engine instead of Docker Desktop.
Docker Engine installation instructions: https://docs.docker.com/engine/install/#server
I0621 22:09:01.326044  231799 cni.go:84] Creating CNI manager for ""
I0621 22:09:01.326130  231799 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0621 22:09:01.326142  231799 start_flags.go:319] Found "bridge CNI" CNI - setting NetworkPlugin=cni
I0621 22:09:01.326859  231799 start.go:340] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/berk:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0621 22:09:01.329113  231799 out.go:177] üëç  Starting "minikube" primary control-plane node in "minikube" cluster
I0621 22:09:01.330857  231799 cache.go:121] Beginning downloading kic base image for docker with docker
I0621 22:09:01.332803  231799 out.go:177] üöú  Pulling base image v0.0.44 ...
I0621 22:09:01.334735  231799 preload.go:132] Checking if preload exists for k8s version v1.30.0 and runtime docker
I0621 22:09:01.334779  231799 preload.go:147] Found local preload: /home/berk/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.30.0-docker-overlay2-amd64.tar.lz4
I0621 22:09:01.334790  231799 cache.go:56] Caching tarball of preloaded images
I0621 22:09:01.334845  231799 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e in local docker daemon
I0621 22:09:01.334976  231799 preload.go:173] Found /home/berk/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.30.0-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0621 22:09:01.334985  231799 cache.go:59] Finished verifying existence of preloaded tar for v1.30.0 on docker
I0621 22:09:01.335570  231799 profile.go:143] Saving config to /home/berk/.minikube/profiles/minikube/config.json ...
I0621 22:09:01.335601  231799 lock.go:35] WriteFile acquiring /home/berk/.minikube/profiles/minikube/config.json: {Name:mka7eefcb8049dad72eaf6aef48eba9b006b0276 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0621 22:09:01.366496  231799 image.go:83] Found gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e in local docker daemon, skipping pull
I0621 22:09:01.366513  231799 cache.go:144] gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e exists in daemon, skipping load
I0621 22:09:01.366655  231799 cache.go:194] Successfully downloaded all kic artifacts
I0621 22:09:01.366855  231799 start.go:360] acquireMachinesLock for minikube: {Name:mk3929f6225e20cd01b954bc7f871ed59a86c0c6 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0621 22:09:01.366943  231799 start.go:364] duration metric: took 74.703¬µs to acquireMachinesLock for "minikube"
I0621 22:09:01.366993  231799 start.go:93] Provisioning new machine with config: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/berk:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} &{Name: IP: Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0621 22:09:01.367087  231799 start.go:125] createHost starting for "" (driver="docker")
I0621 22:09:01.370921  231799 out.go:204] üî•  Creating docker container (CPUs=2, Memory=2200MB) ...
I0621 22:09:01.371314  231799 start.go:159] libmachine.API.Create for "minikube" (driver="docker")
I0621 22:09:01.371463  231799 client.go:168] LocalClient.Create starting
I0621 22:09:01.372126  231799 main.go:141] libmachine: Reading certificate data from /home/berk/.minikube/certs/ca.pem
I0621 22:09:01.372201  231799 main.go:141] libmachine: Decoding PEM data...
I0621 22:09:01.372212  231799 main.go:141] libmachine: Parsing certificate...
I0621 22:09:01.372557  231799 main.go:141] libmachine: Reading certificate data from /home/berk/.minikube/certs/cert.pem
I0621 22:09:01.372618  231799 main.go:141] libmachine: Decoding PEM data...
I0621 22:09:01.372628  231799 main.go:141] libmachine: Parsing certificate...
I0621 22:09:01.373784  231799 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
W0621 22:09:01.397766  231799 cli_runner.go:211] docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}" returned with exit code 1
I0621 22:09:01.397860  231799 network_create.go:281] running [docker network inspect minikube] to gather additional debugging logs...
I0621 22:09:01.397874  231799 cli_runner.go:164] Run: docker network inspect minikube
W0621 22:09:01.422172  231799 cli_runner.go:211] docker network inspect minikube returned with exit code 1
I0621 22:09:01.422188  231799 network_create.go:284] error running [docker network inspect minikube]: docker network inspect minikube: exit status 1
stdout:
[]

stderr:
Error response from daemon: network minikube not found
I0621 22:09:01.422196  231799 network_create.go:286] output of [docker network inspect minikube]: -- stdout --
[]

-- /stdout --
** stderr ** 
Error response from daemon: network minikube not found

** /stderr **
I0621 22:09:01.422296  231799 cli_runner.go:164] Run: docker network inspect bridge --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0621 22:09:01.447175  231799 network.go:206] using free private subnet 192.168.49.0/24: &{IP:192.168.49.0 Netmask:255.255.255.0 Prefix:24 CIDR:192.168.49.0/24 Gateway:192.168.49.1 ClientMin:192.168.49.2 ClientMax:192.168.49.254 Broadcast:192.168.49.255 IsPrivate:true Interface:{IfaceName: IfaceIPv4: IfaceMTU:0 IfaceMAC:} reservation:0xc000691730}
I0621 22:09:01.447361  231799 network_create.go:124] attempt to create docker network minikube 192.168.49.0/24 with gateway 192.168.49.1 and MTU of 1500 ...
I0621 22:09:01.447438  231799 cli_runner.go:164] Run: docker network create --driver=bridge --subnet=192.168.49.0/24 --gateway=192.168.49.1 -o --ip-masq -o --icc -o com.docker.network.driver.mtu=1500 --label=created_by.minikube.sigs.k8s.io=true --label=name.minikube.sigs.k8s.io=minikube minikube
I0621 22:09:01.511443  231799 network_create.go:108] docker network minikube 192.168.49.0/24 created
I0621 22:09:01.511483  231799 kic.go:121] calculated static IP "192.168.49.2" for the "minikube" container
I0621 22:09:01.511586  231799 cli_runner.go:164] Run: docker ps -a --format {{.Names}}
I0621 22:09:01.535012  231799 cli_runner.go:164] Run: docker volume create minikube --label name.minikube.sigs.k8s.io=minikube --label created_by.minikube.sigs.k8s.io=true
I0621 22:09:01.560733  231799 oci.go:103] Successfully created a docker volume minikube
I0621 22:09:01.560844  231799 cli_runner.go:164] Run: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e -d /var/lib
I0621 22:09:02.882128  231799 cli_runner.go:217] Completed: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e -d /var/lib: (1.321245736s)
I0621 22:09:02.882149  231799 oci.go:107] Successfully prepared a docker volume minikube
I0621 22:09:02.882172  231799 preload.go:132] Checking if preload exists for k8s version v1.30.0 and runtime docker
I0621 22:09:02.882263  231799 kic.go:194] Starting extracting preloaded images to volume ...
I0621 22:09:02.882452  231799 cli_runner.go:164] Run: docker run --rm --entrypoint /usr/bin/tar -v /home/berk/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.30.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e -I lz4 -xf /preloaded.tar -C /extractDir
I0621 22:09:09.166564  231799 cli_runner.go:217] Completed: docker run --rm --entrypoint /usr/bin/tar -v /home/berk/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.30.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e -I lz4 -xf /preloaded.tar -C /extractDir: (6.284078475s)
I0621 22:09:09.166592  231799 kic.go:203] duration metric: took 6.284391987s to extract preloaded images to volume ...
W0621 22:09:09.168542  231799 cgroups_linux.go:77] Your kernel does not support swap limit capabilities or the cgroup is not mounted.
I0621 22:09:09.168684  231799 cli_runner.go:164] Run: docker info --format "'{{json .SecurityOptions}}'"
I0621 22:09:09.268797  231799 cli_runner.go:164] Run: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube --name minikube --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube --network minikube --ip 192.168.49.2 --volume minikube:/var --security-opt apparmor=unconfined --memory=2200mb --cpus=2 -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e
I0621 22:09:09.756819  231799 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Running}}
I0621 22:09:09.799754  231799 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0621 22:09:09.834423  231799 cli_runner.go:164] Run: docker exec minikube stat /var/lib/dpkg/alternatives/iptables
I0621 22:09:09.926132  231799 oci.go:144] the created container "minikube" has a running status.
I0621 22:09:09.926424  231799 kic.go:225] Creating ssh key for kic: /home/berk/.minikube/machines/minikube/id_rsa...
I0621 22:09:10.014294  231799 kic_runner.go:191] docker (temp): /home/berk/.minikube/machines/minikube/id_rsa.pub --> /home/docker/.ssh/authorized_keys (381 bytes)
I0621 22:09:10.054241  231799 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0621 22:09:10.081342  231799 kic_runner.go:93] Run: chown docker:docker /home/docker/.ssh/authorized_keys
I0621 22:09:10.081352  231799 kic_runner.go:114] Args: [docker exec --privileged minikube chown docker:docker /home/docker/.ssh/authorized_keys]
I0621 22:09:10.159649  231799 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0621 22:09:10.182226  231799 machine.go:94] provisionDockerMachine start ...
I0621 22:09:10.182587  231799 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0621 22:09:10.210917  231799 main.go:141] libmachine: Using SSH client type: native
I0621 22:09:10.211212  231799 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82d6e0] 0x830440 <nil>  [] 0s} 127.0.0.1 58058 <nil> <nil>}
I0621 22:09:10.211223  231799 main.go:141] libmachine: About to run SSH command:
hostname
I0621 22:09:10.212328  231799 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: EOF
I0621 22:09:13.371744  231799 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0621 22:09:13.372192  231799 ubuntu.go:169] provisioning hostname "minikube"
I0621 22:09:13.372356  231799 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0621 22:09:13.404164  231799 main.go:141] libmachine: Using SSH client type: native
I0621 22:09:13.404411  231799 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82d6e0] 0x830440 <nil>  [] 0s} 127.0.0.1 58058 <nil> <nil>}
I0621 22:09:13.404418  231799 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0621 22:09:13.569856  231799 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0621 22:09:13.570121  231799 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0621 22:09:13.601997  231799 main.go:141] libmachine: Using SSH client type: native
I0621 22:09:13.602215  231799 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82d6e0] 0x830440 <nil>  [] 0s} 127.0.0.1 58058 <nil> <nil>}
I0621 22:09:13.602224  231799 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0621 22:09:13.753997  231799 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0621 22:09:13.754061  231799 ubuntu.go:175] set auth options {CertDir:/home/berk/.minikube CaCertPath:/home/berk/.minikube/certs/ca.pem CaPrivateKeyPath:/home/berk/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/berk/.minikube/machines/server.pem ServerKeyPath:/home/berk/.minikube/machines/server-key.pem ClientKeyPath:/home/berk/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/berk/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/berk/.minikube}
I0621 22:09:13.754082  231799 ubuntu.go:177] setting up certificates
I0621 22:09:13.754217  231799 provision.go:84] configureAuth start
I0621 22:09:13.754391  231799 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0621 22:09:13.797377  231799 provision.go:143] copyHostCerts
I0621 22:09:13.797542  231799 exec_runner.go:144] found /home/berk/.minikube/ca.pem, removing ...
I0621 22:09:13.797592  231799 exec_runner.go:203] rm: /home/berk/.minikube/ca.pem
I0621 22:09:13.797693  231799 exec_runner.go:151] cp: /home/berk/.minikube/certs/ca.pem --> /home/berk/.minikube/ca.pem (1074 bytes)
I0621 22:09:13.797799  231799 exec_runner.go:144] found /home/berk/.minikube/cert.pem, removing ...
I0621 22:09:13.797802  231799 exec_runner.go:203] rm: /home/berk/.minikube/cert.pem
I0621 22:09:13.797830  231799 exec_runner.go:151] cp: /home/berk/.minikube/certs/cert.pem --> /home/berk/.minikube/cert.pem (1115 bytes)
I0621 22:09:13.797890  231799 exec_runner.go:144] found /home/berk/.minikube/key.pem, removing ...
I0621 22:09:13.797893  231799 exec_runner.go:203] rm: /home/berk/.minikube/key.pem
I0621 22:09:13.797918  231799 exec_runner.go:151] cp: /home/berk/.minikube/certs/key.pem --> /home/berk/.minikube/key.pem (1679 bytes)
I0621 22:09:13.797964  231799 provision.go:117] generating server cert: /home/berk/.minikube/machines/server.pem ca-key=/home/berk/.minikube/certs/ca.pem private-key=/home/berk/.minikube/certs/ca-key.pem org=berk.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I0621 22:09:13.919546  231799 provision.go:177] copyRemoteCerts
I0621 22:09:13.919618  231799 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0621 22:09:13.919685  231799 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0621 22:09:13.954447  231799 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:58058 SSHKeyPath:/home/berk/.minikube/machines/minikube/id_rsa Username:docker}
I0621 22:09:14.072353  231799 ssh_runner.go:362] scp /home/berk/.minikube/machines/server.pem --> /etc/docker/server.pem (1172 bytes)
I0621 22:09:14.109676  231799 ssh_runner.go:362] scp /home/berk/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I0621 22:09:14.145957  231799 ssh_runner.go:362] scp /home/berk/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1074 bytes)
I0621 22:09:14.190708  231799 provision.go:87] duration metric: took 436.475931ms to configureAuth
I0621 22:09:14.190725  231799 ubuntu.go:193] setting minikube options for container-runtime
I0621 22:09:14.190971  231799 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.30.0
I0621 22:09:14.191096  231799 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0621 22:09:14.229940  231799 main.go:141] libmachine: Using SSH client type: native
I0621 22:09:14.230087  231799 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82d6e0] 0x830440 <nil>  [] 0s} 127.0.0.1 58058 <nil> <nil>}
I0621 22:09:14.230117  231799 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0621 22:09:14.392962  231799 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0621 22:09:14.392977  231799 ubuntu.go:71] root file system type: overlay
I0621 22:09:14.393077  231799 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0621 22:09:14.393155  231799 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0621 22:09:14.420798  231799 main.go:141] libmachine: Using SSH client type: native
I0621 22:09:14.420935  231799 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82d6e0] 0x830440 <nil>  [] 0s} 127.0.0.1 58058 <nil> <nil>}
I0621 22:09:14.420987  231799 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0621 22:09:14.599490  231799 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0621 22:09:14.599733  231799 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0621 22:09:14.634301  231799 main.go:141] libmachine: Using SSH client type: native
I0621 22:09:14.634444  231799 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82d6e0] 0x830440 <nil>  [] 0s} 127.0.0.1 58058 <nil> <nil>}
I0621 22:09:14.634455  231799 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0621 22:09:15.716633  231799 main.go:141] libmachine: SSH cmd err, output: <nil>: --- /lib/systemd/system/docker.service	2024-04-30 11:46:26.000000000 +0000
+++ /lib/systemd/system/docker.service.new	2024-06-21 19:09:14.578454823 +0000
@@ -1,46 +1,49 @@
 [Unit]
 Description=Docker Application Container Engine
 Documentation=https://docs.docker.com
-After=network-online.target docker.socket firewalld.service containerd.service time-set.target
-Wants=network-online.target containerd.service
+BindsTo=containerd.service
+After=network-online.target firewalld.service containerd.service
+Wants=network-online.target
 Requires=docker.socket
+StartLimitBurst=3
+StartLimitIntervalSec=60
 
 [Service]
 Type=notify
-# the default is not to use systemd for cgroups because the delegate issues still
-# exists and systemd currently does not support the cgroup feature set required
-# for containers run by docker
-ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock
-ExecReload=/bin/kill -s HUP $MAINPID
-TimeoutStartSec=0
-RestartSec=2
-Restart=always
+Restart=on-failure
 
-# Note that StartLimit* options were moved from "Service" to "Unit" in systemd 229.
-# Both the old, and new location are accepted by systemd 229 and up, so using the old location
-# to make them work for either version of systemd.
-StartLimitBurst=3
 
-# Note that StartLimitInterval was renamed to StartLimitIntervalSec in systemd 230.
-# Both the old, and new name are accepted by systemd 230 and up, so using the old name to make
-# this option work for either version of systemd.
-StartLimitInterval=60s
+
+# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
+# The base configuration already specifies an 'ExecStart=...' command. The first directive
+# here is to clear out that command inherited from the base configuration. Without this,
+# the command from the base configuration and the command specified here are treated as
+# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
+# will catch this invalid input and refuse to start the service with an error like:
+#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.
+
+# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
+# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
+ExecStart=
+ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
+ExecReload=/bin/kill -s HUP $MAINPID
 
 # Having non-zero Limit*s causes performance problems due to accounting overhead
 # in the kernel. We recommend using cgroups to do container-local accounting.
+LimitNOFILE=infinity
 LimitNPROC=infinity
 LimitCORE=infinity
 
-# Comment TasksMax if your systemd version does not support it.
-# Only systemd 226 and above support this option.
+# Uncomment TasksMax if your systemd version supports it.
+# Only systemd 226 and above support this version.
 TasksMax=infinity
+TimeoutStartSec=0
 
 # set delegate yes so that systemd does not reset the cgroups of docker containers
 Delegate=yes
 
 # kill only the docker process, not all processes in the cgroup
 KillMode=process
-OOMScoreAdjust=-500
 
 [Install]
 WantedBy=multi-user.target
Synchronizing state of docker.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable docker

I0621 22:09:15.716649  231799 machine.go:97] duration metric: took 5.534409748s to provisionDockerMachine
I0621 22:09:15.716661  231799 client.go:171] duration metric: took 14.34519211s to LocalClient.Create
I0621 22:09:15.716702  231799 start.go:167] duration metric: took 14.345388217s to libmachine.API.Create "minikube"
I0621 22:09:15.716825  231799 start.go:293] postStartSetup for "minikube" (driver="docker")
I0621 22:09:15.716836  231799 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0621 22:09:15.716908  231799 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0621 22:09:15.716966  231799 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0621 22:09:15.746889  231799 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:58058 SSHKeyPath:/home/berk/.minikube/machines/minikube/id_rsa Username:docker}
I0621 22:09:15.871623  231799 ssh_runner.go:195] Run: cat /etc/os-release
I0621 22:09:15.879165  231799 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0621 22:09:15.879246  231799 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0621 22:09:15.879286  231799 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0621 22:09:15.879295  231799 info.go:137] Remote host: Ubuntu 22.04.4 LTS
I0621 22:09:15.879352  231799 filesync.go:126] Scanning /home/berk/.minikube/addons for local assets ...
I0621 22:09:15.879546  231799 filesync.go:126] Scanning /home/berk/.minikube/files for local assets ...
I0621 22:09:15.879624  231799 start.go:296] duration metric: took 162.790348ms for postStartSetup
I0621 22:09:15.880068  231799 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0621 22:09:15.917064  231799 profile.go:143] Saving config to /home/berk/.minikube/profiles/minikube/config.json ...
I0621 22:09:15.917430  231799 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0621 22:09:15.917490  231799 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0621 22:09:15.945008  231799 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:58058 SSHKeyPath:/home/berk/.minikube/machines/minikube/id_rsa Username:docker}
I0621 22:09:16.043857  231799 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0621 22:09:16.050926  231799 start.go:128] duration metric: took 14.683782829s to createHost
I0621 22:09:16.050939  231799 start.go:83] releasing machines lock for "minikube", held for 14.683988538s
I0621 22:09:16.051052  231799 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0621 22:09:16.083283  231799 ssh_runner.go:195] Run: cat /version.json
I0621 22:09:16.083344  231799 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0621 22:09:16.083363  231799 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0621 22:09:16.083482  231799 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0621 22:09:16.115428  231799 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:58058 SSHKeyPath:/home/berk/.minikube/machines/minikube/id_rsa Username:docker}
I0621 22:09:16.125791  231799 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:58058 SSHKeyPath:/home/berk/.minikube/machines/minikube/id_rsa Username:docker}
I0621 22:09:16.575971  231799 ssh_runner.go:195] Run: systemctl --version
I0621 22:09:16.583102  231799 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0621 22:09:16.590420  231799 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I0621 22:09:16.627693  231799 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I0621 22:09:16.627784  231799 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0621 22:09:16.663587  231799 cni.go:262] disabled [/etc/cni/net.d/100-crio-bridge.conf, /etc/cni/net.d/87-podman-bridge.conflist] bridge cni config(s)
I0621 22:09:16.663670  231799 start.go:494] detecting cgroup driver to use...
I0621 22:09:16.663696  231799 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0621 22:09:16.664300  231799 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0621 22:09:16.684485  231799 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.9"|' /etc/containerd/config.toml"
I0621 22:09:16.698849  231799 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0621 22:09:16.711889  231799 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I0621 22:09:16.711973  231799 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0621 22:09:16.724152  231799 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0621 22:09:16.736368  231799 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0621 22:09:16.750005  231799 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0621 22:09:16.762584  231799 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0621 22:09:16.775657  231799 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0621 22:09:16.790810  231799 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0621 22:09:16.803837  231799 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0621 22:09:16.817104  231799 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0621 22:09:16.829047  231799 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0621 22:09:16.840879  231799 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0621 22:09:16.963436  231799 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0621 22:09:17.093637  231799 start.go:494] detecting cgroup driver to use...
I0621 22:09:17.093675  231799 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0621 22:09:17.093999  231799 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0621 22:09:17.111449  231799 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0621 22:09:17.111550  231799 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0621 22:09:17.128794  231799 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0621 22:09:17.154219  231799 ssh_runner.go:195] Run: which cri-dockerd
I0621 22:09:17.160408  231799 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0621 22:09:17.174446  231799 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (189 bytes)
I0621 22:09:17.205244  231799 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0621 22:09:17.339720  231799 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0621 22:09:17.494267  231799 docker.go:574] configuring docker to use "cgroupfs" as cgroup driver...
I0621 22:09:17.494392  231799 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I0621 22:09:17.514464  231799 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0621 22:09:17.629814  231799 ssh_runner.go:195] Run: sudo systemctl restart docker
I0621 22:09:18.027666  231799 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0621 22:09:18.041278  231799 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0621 22:09:18.055537  231799 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0621 22:09:18.174357  231799 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0621 22:09:18.298118  231799 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0621 22:09:18.432079  231799 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0621 22:09:18.455383  231799 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0621 22:09:18.474413  231799 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0621 22:09:18.603181  231799 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0621 22:09:18.797510  231799 start.go:541] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0621 22:09:18.797676  231799 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0621 22:09:18.804327  231799 start.go:562] Will wait 60s for crictl version
I0621 22:09:18.804421  231799 ssh_runner.go:195] Run: which crictl
I0621 22:09:18.811278  231799 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0621 22:09:18.974087  231799 start.go:578] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  26.1.1
RuntimeApiVersion:  v1
I0621 22:09:18.974182  231799 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0621 22:09:19.089578  231799 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0621 22:09:19.125305  231799 out.go:204] üê≥  Preparing Kubernetes v1.30.0 on Docker 26.1.1 ...
I0621 22:09:19.125486  231799 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0621 22:09:19.154082  231799 ssh_runner.go:195] Run: grep 192.168.49.1	host.minikube.internal$ /etc/hosts
I0621 22:09:19.161482  231799 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.49.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0621 22:09:19.179512  231799 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0621 22:09:19.215121  231799 kubeadm.go:877] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/berk:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I0621 22:09:19.215453  231799 preload.go:132] Checking if preload exists for k8s version v1.30.0 and runtime docker
I0621 22:09:19.215634  231799 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0621 22:09:19.242553  231799 docker.go:685] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.30.0
registry.k8s.io/kube-scheduler:v1.30.0
registry.k8s.io/kube-controller-manager:v1.30.0
registry.k8s.io/kube-proxy:v1.30.0
registry.k8s.io/etcd:3.5.12-0
registry.k8s.io/coredns/coredns:v1.11.1
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0621 22:09:19.242605  231799 docker.go:615] Images already preloaded, skipping extraction
I0621 22:09:19.242798  231799 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0621 22:09:19.263204  231799 docker.go:685] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.30.0
registry.k8s.io/kube-controller-manager:v1.30.0
registry.k8s.io/kube-scheduler:v1.30.0
registry.k8s.io/kube-proxy:v1.30.0
registry.k8s.io/etcd:3.5.12-0
registry.k8s.io/coredns/coredns:v1.11.1
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0621 22:09:19.263353  231799 cache_images.go:84] Images are preloaded, skipping loading
I0621 22:09:19.263446  231799 kubeadm.go:928] updating node { 192.168.49.2 8443 v1.30.0 docker true true} ...
I0621 22:09:19.263723  231799 kubeadm.go:940] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.30.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0621 22:09:19.263815  231799 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0621 22:09:19.466113  231799 cni.go:84] Creating CNI manager for ""
I0621 22:09:19.466124  231799 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0621 22:09:19.466182  231799 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0621 22:09:19.466217  231799 kubeadm.go:181] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.30.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0621 22:09:19.466370  231799 kubeadm.go:187] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.30.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0621 22:09:19.466458  231799 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.30.0
I0621 22:09:19.480913  231799 binaries.go:44] Found k8s binaries, skipping transfer
I0621 22:09:19.481019  231799 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0621 22:09:19.492851  231799 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I0621 22:09:19.516771  231799 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0621 22:09:19.537902  231799 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2150 bytes)
I0621 22:09:19.560501  231799 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0621 22:09:19.565848  231799 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0621 22:09:19.579418  231799 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0621 22:09:19.697809  231799 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0621 22:09:19.717464  231799 certs.go:68] Setting up /home/berk/.minikube/profiles/minikube for IP: 192.168.49.2
I0621 22:09:19.717495  231799 certs.go:194] generating shared ca certs ...
I0621 22:09:19.717584  231799 certs.go:226] acquiring lock for ca certs: {Name:mk2113c7795b6001049c8c961cd14ad96a5cb772 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0621 22:09:19.717796  231799 certs.go:235] skipping valid "minikubeCA" ca cert: /home/berk/.minikube/ca.key
I0621 22:09:19.717882  231799 certs.go:235] skipping valid "proxyClientCA" ca cert: /home/berk/.minikube/proxy-client-ca.key
I0621 22:09:19.717909  231799 certs.go:256] generating profile certs ...
I0621 22:09:19.717998  231799 certs.go:363] generating signed profile cert for "minikube-user": /home/berk/.minikube/profiles/minikube/client.key
I0621 22:09:19.718194  231799 crypto.go:68] Generating cert /home/berk/.minikube/profiles/minikube/client.crt with IP's: []
I0621 22:09:19.956239  231799 crypto.go:156] Writing cert to /home/berk/.minikube/profiles/minikube/client.crt ...
I0621 22:09:19.956345  231799 lock.go:35] WriteFile acquiring /home/berk/.minikube/profiles/minikube/client.crt: {Name:mk8551f32b39d7578600bc2d401ca2c272b2c1e8 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0621 22:09:19.956717  231799 crypto.go:164] Writing key to /home/berk/.minikube/profiles/minikube/client.key ...
I0621 22:09:19.956738  231799 lock.go:35] WriteFile acquiring /home/berk/.minikube/profiles/minikube/client.key: {Name:mkdf8c9eeac1a93cba36a48726bbb072c78ebc7c Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0621 22:09:19.957115  231799 certs.go:363] generating signed profile cert for "minikube": /home/berk/.minikube/profiles/minikube/apiserver.key.7fb57e3c
I0621 22:09:19.957136  231799 crypto.go:68] Generating cert /home/berk/.minikube/profiles/minikube/apiserver.crt.7fb57e3c with IP's: [10.96.0.1 127.0.0.1 10.0.0.1 192.168.49.2]
I0621 22:09:20.026578  231799 crypto.go:156] Writing cert to /home/berk/.minikube/profiles/minikube/apiserver.crt.7fb57e3c ...
I0621 22:09:20.026594  231799 lock.go:35] WriteFile acquiring /home/berk/.minikube/profiles/minikube/apiserver.crt.7fb57e3c: {Name:mk82e0234c3285016d20a6313f547c03a7a5079d Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0621 22:09:20.026836  231799 crypto.go:164] Writing key to /home/berk/.minikube/profiles/minikube/apiserver.key.7fb57e3c ...
I0621 22:09:20.026844  231799 lock.go:35] WriteFile acquiring /home/berk/.minikube/profiles/minikube/apiserver.key.7fb57e3c: {Name:mk2d0dbfbfeb5a8e46565e879f112c5905ff54c0 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0621 22:09:20.026958  231799 certs.go:381] copying /home/berk/.minikube/profiles/minikube/apiserver.crt.7fb57e3c -> /home/berk/.minikube/profiles/minikube/apiserver.crt
I0621 22:09:20.027217  231799 certs.go:385] copying /home/berk/.minikube/profiles/minikube/apiserver.key.7fb57e3c -> /home/berk/.minikube/profiles/minikube/apiserver.key
I0621 22:09:20.027299  231799 certs.go:363] generating signed profile cert for "aggregator": /home/berk/.minikube/profiles/minikube/proxy-client.key
I0621 22:09:20.027314  231799 crypto.go:68] Generating cert /home/berk/.minikube/profiles/minikube/proxy-client.crt with IP's: []
I0621 22:09:20.269012  231799 crypto.go:156] Writing cert to /home/berk/.minikube/profiles/minikube/proxy-client.crt ...
I0621 22:09:20.269024  231799 lock.go:35] WriteFile acquiring /home/berk/.minikube/profiles/minikube/proxy-client.crt: {Name:mk397843e83d1cc1bf578e55fa8f26db3fcbb385 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0621 22:09:20.269239  231799 crypto.go:164] Writing key to /home/berk/.minikube/profiles/minikube/proxy-client.key ...
I0621 22:09:20.269244  231799 lock.go:35] WriteFile acquiring /home/berk/.minikube/profiles/minikube/proxy-client.key: {Name:mkdb3911306efd923f42ecc705c00eb8748cafa4 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0621 22:09:20.269520  231799 certs.go:484] found cert: /home/berk/.minikube/certs/ca-key.pem (1675 bytes)
I0621 22:09:20.269551  231799 certs.go:484] found cert: /home/berk/.minikube/certs/ca.pem (1074 bytes)
I0621 22:09:20.269573  231799 certs.go:484] found cert: /home/berk/.minikube/certs/cert.pem (1115 bytes)
I0621 22:09:20.269592  231799 certs.go:484] found cert: /home/berk/.minikube/certs/key.pem (1679 bytes)
I0621 22:09:20.273779  231799 ssh_runner.go:362] scp /home/berk/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0621 22:09:20.302611  231799 ssh_runner.go:362] scp /home/berk/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I0621 22:09:20.333323  231799 ssh_runner.go:362] scp /home/berk/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0621 22:09:20.363077  231799 ssh_runner.go:362] scp /home/berk/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I0621 22:09:20.392481  231799 ssh_runner.go:362] scp /home/berk/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0621 22:09:20.423820  231799 ssh_runner.go:362] scp /home/berk/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I0621 22:09:20.459483  231799 ssh_runner.go:362] scp /home/berk/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0621 22:09:20.495575  231799 ssh_runner.go:362] scp /home/berk/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I0621 22:09:20.537309  231799 ssh_runner.go:362] scp /home/berk/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0621 22:09:20.571373  231799 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0621 22:09:20.601649  231799 ssh_runner.go:195] Run: openssl version
I0621 22:09:20.617324  231799 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0621 22:09:20.636507  231799 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0621 22:09:20.643766  231799 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Jun 21 06:49 /usr/share/ca-certificates/minikubeCA.pem
I0621 22:09:20.643897  231799 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0621 22:09:20.654678  231799 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0621 22:09:20.670509  231799 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0621 22:09:20.677305  231799 certs.go:399] 'apiserver-kubelet-client' cert doesn't exist, likely first start: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt: Process exited with status 1
stdout:

stderr:
stat: cannot statx '/var/lib/minikube/certs/apiserver-kubelet-client.crt': No such file or directory
I0621 22:09:20.677516  231799 kubeadm.go:391] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/berk:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0621 22:09:20.677675  231799 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0621 22:09:20.714178  231799 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0621 22:09:20.724863  231799 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0621 22:09:20.735124  231799 kubeadm.go:213] ignoring SystemVerification for kubeadm because of docker driver
I0621 22:09:20.735231  231799 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0621 22:09:20.745733  231799 kubeadm.go:154] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I0621 22:09:20.745742  231799 kubeadm.go:156] found existing configuration files:

I0621 22:09:20.745815  231799 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I0621 22:09:20.755830  231799 kubeadm.go:162] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/admin.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/admin.conf: No such file or directory
I0621 22:09:20.755908  231799 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/admin.conf
I0621 22:09:20.766217  231799 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I0621 22:09:20.776624  231799 kubeadm.go:162] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/kubelet.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/kubelet.conf: No such file or directory
I0621 22:09:20.776695  231799 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/kubelet.conf
I0621 22:09:20.786292  231799 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I0621 22:09:20.796780  231799 kubeadm.go:162] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/controller-manager.conf: No such file or directory
I0621 22:09:20.796854  231799 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I0621 22:09:20.807239  231799 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I0621 22:09:20.819782  231799 kubeadm.go:162] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/scheduler.conf: No such file or directory
I0621 22:09:20.819895  231799 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I0621 22:09:20.832041  231799 ssh_runner.go:286] Start: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.30.0:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,NumCPU,Mem,SystemVerification,FileContent--proc-sys-net-bridge-bridge-nf-call-iptables"
I0621 22:09:20.880877  231799 kubeadm.go:309] [init] Using Kubernetes version: v1.30.0
I0621 22:09:20.880914  231799 kubeadm.go:309] [preflight] Running pre-flight checks
I0621 22:09:21.006129  231799 kubeadm.go:309] [preflight] Pulling images required for setting up a Kubernetes cluster
I0621 22:09:21.006240  231799 kubeadm.go:309] [preflight] This might take a minute or two, depending on the speed of your internet connection
I0621 22:09:21.006372  231799 kubeadm.go:309] [preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
I0621 22:09:21.310388  231799 kubeadm.go:309] [certs] Using certificateDir folder "/var/lib/minikube/certs"
I0621 22:09:21.312434  231799 out.go:204]     ‚ñ™ Generating certificates and keys ...
I0621 22:09:21.312736  231799 kubeadm.go:309] [certs] Using existing ca certificate authority
I0621 22:09:21.312849  231799 kubeadm.go:309] [certs] Using existing apiserver certificate and key on disk
I0621 22:09:21.510406  231799 kubeadm.go:309] [certs] Generating "apiserver-kubelet-client" certificate and key
I0621 22:09:21.665883  231799 kubeadm.go:309] [certs] Generating "front-proxy-ca" certificate and key
I0621 22:09:21.812527  231799 kubeadm.go:309] [certs] Generating "front-proxy-client" certificate and key
I0621 22:09:22.013376  231799 kubeadm.go:309] [certs] Generating "etcd/ca" certificate and key
I0621 22:09:22.152338  231799 kubeadm.go:309] [certs] Generating "etcd/server" certificate and key
I0621 22:09:22.152465  231799 kubeadm.go:309] [certs] etcd/server serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I0621 22:09:22.446712  231799 kubeadm.go:309] [certs] Generating "etcd/peer" certificate and key
I0621 22:09:22.446847  231799 kubeadm.go:309] [certs] etcd/peer serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I0621 22:09:22.613437  231799 kubeadm.go:309] [certs] Generating "etcd/healthcheck-client" certificate and key
I0621 22:09:22.689347  231799 kubeadm.go:309] [certs] Generating "apiserver-etcd-client" certificate and key
I0621 22:09:22.851351  231799 kubeadm.go:309] [certs] Generating "sa" key and public key
I0621 22:09:22.851413  231799 kubeadm.go:309] [kubeconfig] Using kubeconfig folder "/etc/kubernetes"
I0621 22:09:23.069409  231799 kubeadm.go:309] [kubeconfig] Writing "admin.conf" kubeconfig file
I0621 22:09:23.207029  231799 kubeadm.go:309] [kubeconfig] Writing "super-admin.conf" kubeconfig file
I0621 22:09:23.330711  231799 kubeadm.go:309] [kubeconfig] Writing "kubelet.conf" kubeconfig file
I0621 22:09:23.506166  231799 kubeadm.go:309] [kubeconfig] Writing "controller-manager.conf" kubeconfig file
I0621 22:09:23.626030  231799 kubeadm.go:309] [kubeconfig] Writing "scheduler.conf" kubeconfig file
I0621 22:09:23.627999  231799 kubeadm.go:309] [etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
I0621 22:09:23.631665  231799 kubeadm.go:309] [control-plane] Using manifest folder "/etc/kubernetes/manifests"
I0621 22:09:23.640662  231799 out.go:204]     ‚ñ™ Booting up control plane ...
I0621 22:09:23.640943  231799 kubeadm.go:309] [control-plane] Creating static Pod manifest for "kube-apiserver"
I0621 22:09:23.641061  231799 kubeadm.go:309] [control-plane] Creating static Pod manifest for "kube-controller-manager"
I0621 22:09:23.641116  231799 kubeadm.go:309] [control-plane] Creating static Pod manifest for "kube-scheduler"
I0621 22:09:23.647806  231799 kubeadm.go:309] [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
I0621 22:09:23.649246  231799 kubeadm.go:309] [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
I0621 22:09:23.649307  231799 kubeadm.go:309] [kubelet-start] Starting the kubelet
I0621 22:09:23.792097  231799 kubeadm.go:309] [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests"
I0621 22:09:23.792202  231799 kubeadm.go:309] [kubelet-check] Waiting for a healthy kubelet. This can take up to 4m0s
I0621 22:09:24.294109  231799 kubeadm.go:309] [kubelet-check] The kubelet is healthy after 502.123072ms
I0621 22:09:24.294214  231799 kubeadm.go:309] [api-check] Waiting for a healthy API server. This can take up to 4m0s
I0621 22:09:29.794616  231799 kubeadm.go:309] [api-check] The API server is healthy after 5.50170428s
I0621 22:09:29.810691  231799 kubeadm.go:309] [upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
I0621 22:09:29.823685  231799 kubeadm.go:309] [kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
I0621 22:09:29.847688  231799 kubeadm.go:309] [upload-certs] Skipping phase. Please see --upload-certs
I0621 22:09:29.847900  231799 kubeadm.go:309] [mark-control-plane] Marking the node minikube as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
I0621 22:09:29.857653  231799 kubeadm.go:309] [bootstrap-token] Using token: vp5sne.jioyq2d5rfbpm7y5
I0621 22:09:29.860122  231799 out.go:204]     ‚ñ™ Configuring RBAC rules ...
I0621 22:09:29.860426  231799 kubeadm.go:309] [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
I0621 22:09:29.864844  231799 kubeadm.go:309] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
I0621 22:09:29.874274  231799 kubeadm.go:309] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
I0621 22:09:29.878978  231799 kubeadm.go:309] [bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
I0621 22:09:29.886218  231799 kubeadm.go:309] [bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
I0621 22:09:29.890383  231799 kubeadm.go:309] [bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
I0621 22:09:30.201469  231799 kubeadm.go:309] [kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
I0621 22:09:30.687284  231799 kubeadm.go:309] [addons] Applied essential addon: CoreDNS
I0621 22:09:31.200881  231799 kubeadm.go:309] [addons] Applied essential addon: kube-proxy
I0621 22:09:31.201758  231799 kubeadm.go:309] 
I0621 22:09:31.201817  231799 kubeadm.go:309] Your Kubernetes control-plane has initialized successfully!
I0621 22:09:31.201820  231799 kubeadm.go:309] 
I0621 22:09:31.201930  231799 kubeadm.go:309] To start using your cluster, you need to run the following as a regular user:
I0621 22:09:31.201939  231799 kubeadm.go:309] 
I0621 22:09:31.201963  231799 kubeadm.go:309]   mkdir -p $HOME/.kube
I0621 22:09:31.202060  231799 kubeadm.go:309]   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
I0621 22:09:31.202138  231799 kubeadm.go:309]   sudo chown $(id -u):$(id -g) $HOME/.kube/config
I0621 22:09:31.202143  231799 kubeadm.go:309] 
I0621 22:09:31.202193  231799 kubeadm.go:309] Alternatively, if you are the root user, you can run:
I0621 22:09:31.202196  231799 kubeadm.go:309] 
I0621 22:09:31.202240  231799 kubeadm.go:309]   export KUBECONFIG=/etc/kubernetes/admin.conf
I0621 22:09:31.202243  231799 kubeadm.go:309] 
I0621 22:09:31.202307  231799 kubeadm.go:309] You should now deploy a pod network to the cluster.
I0621 22:09:31.202403  231799 kubeadm.go:309] Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
I0621 22:09:31.202525  231799 kubeadm.go:309]   https://kubernetes.io/docs/concepts/cluster-administration/addons/
I0621 22:09:31.202532  231799 kubeadm.go:309] 
I0621 22:09:31.202610  231799 kubeadm.go:309] You can now join any number of control-plane nodes by copying certificate authorities
I0621 22:09:31.202716  231799 kubeadm.go:309] and service account keys on each node and then running the following as root:
I0621 22:09:31.202722  231799 kubeadm.go:309] 
I0621 22:09:31.202796  231799 kubeadm.go:309]   kubeadm join control-plane.minikube.internal:8443 --token vp5sne.jioyq2d5rfbpm7y5 \
I0621 22:09:31.202935  231799 kubeadm.go:309] 	--discovery-token-ca-cert-hash sha256:eba93150fc0e9df90fbdfd6c041b76b856e9471774b51fd09161baf49148453d \
I0621 22:09:31.202957  231799 kubeadm.go:309] 	--control-plane 
I0621 22:09:31.202961  231799 kubeadm.go:309] 
I0621 22:09:31.203074  231799 kubeadm.go:309] Then you can join any number of worker nodes by running the following on each as root:
I0621 22:09:31.203081  231799 kubeadm.go:309] 
I0621 22:09:31.203238  231799 kubeadm.go:309] kubeadm join control-plane.minikube.internal:8443 --token vp5sne.jioyq2d5rfbpm7y5 \
I0621 22:09:31.203430  231799 kubeadm.go:309] 	--discovery-token-ca-cert-hash sha256:eba93150fc0e9df90fbdfd6c041b76b856e9471774b51fd09161baf49148453d 
I0621 22:09:31.205002  231799 kubeadm.go:309] 	[WARNING Swap]: swap is supported for cgroup v2 only; the NodeSwap feature gate of the kubelet is beta but disabled by default
I0621 22:09:31.205269  231799 kubeadm.go:309] 	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
I0621 22:09:31.205285  231799 cni.go:84] Creating CNI manager for ""
I0621 22:09:31.205296  231799 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0621 22:09:31.207407  231799 out.go:177] üîó  Configuring bridge CNI (Container Networking Interface) ...
I0621 22:09:31.209506  231799 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I0621 22:09:31.222725  231799 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (496 bytes)
I0621 22:09:31.249871  231799 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I0621 22:09:31.250064  231799 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.30.0/kubectl create clusterrolebinding minikube-rbac --clusterrole=cluster-admin --serviceaccount=kube-system:default --kubeconfig=/var/lib/minikube/kubeconfig
I0621 22:09:31.250768  231799 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.30.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig label --overwrite nodes minikube minikube.k8s.io/updated_at=2024_06_21T22_09_31_0700 minikube.k8s.io/version=v1.33.1 minikube.k8s.io/commit=5883c09216182566a63dff4c326a6fc9ed2982ff minikube.k8s.io/name=minikube minikube.k8s.io/primary=true
I0621 22:09:31.334016  231799 kubeadm.go:1107] duration metric: took 84.067626ms to wait for elevateKubeSystemPrivileges
I0621 22:09:31.334047  231799 ops.go:34] apiserver oom_adj: -16
W0621 22:09:31.387072  231799 kubeadm.go:286] apiserver tunnel failed: apiserver port not set
I0621 22:09:31.387087  231799 kubeadm.go:393] duration metric: took 10.710839933s to StartCluster
I0621 22:09:31.387105  231799 settings.go:142] acquiring lock: {Name:mk86a4f067b1b95fb9caf65f5995c0e51a7bbceb Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0621 22:09:31.387187  231799 settings.go:150] Updating kubeconfig:  /home/berk/.kube/config
I0621 22:09:31.391724  231799 lock.go:35] WriteFile acquiring /home/berk/.kube/config: {Name:mke083100ee282522449dcf880b2181e946de681 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0621 22:09:31.392126  231799 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.30.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I0621 22:09:31.392232  231799 start.go:234] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0621 22:09:31.394576  231799 out.go:177] üîé  Verifying Kubernetes components...
I0621 22:09:31.392453  231799 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.30.0
I0621 22:09:31.392601  231799 addons.go:502] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volumesnapshots:false yakd:false]
I0621 22:09:31.396894  231799 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0621 22:09:31.396912  231799 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0621 22:09:31.396948  231799 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0621 22:09:31.397092  231799 addons.go:234] Setting addon storage-provisioner=true in "minikube"
I0621 22:09:31.397171  231799 host.go:66] Checking if "minikube" exists ...
I0621 22:09:31.397183  231799 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0621 22:09:31.397532  231799 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0621 22:09:31.397747  231799 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0621 22:09:31.447766  231799 out.go:177]     ‚ñ™ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0621 22:09:31.449907  231799 addons.go:426] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0621 22:09:31.449917  231799 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0621 22:09:31.449998  231799 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0621 22:09:31.447488  231799 addons.go:234] Setting addon default-storageclass=true in "minikube"
I0621 22:09:31.450205  231799 host.go:66] Checking if "minikube" exists ...
I0621 22:09:31.450825  231799 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0621 22:09:31.493778  231799 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:58058 SSHKeyPath:/home/berk/.minikube/machines/minikube/id_rsa Username:docker}
I0621 22:09:31.496021  231799 addons.go:426] installing /etc/kubernetes/addons/storageclass.yaml
I0621 22:09:31.496033  231799 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0621 22:09:31.496184  231799 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0621 22:09:31.527445  231799 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:58058 SSHKeyPath:/home/berk/.minikube/machines/minikube/id_rsa Username:docker}
I0621 22:09:31.573448  231799 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.30.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml | sed -e '/^        forward . \/etc\/resolv.conf.*/i \        hosts {\n           192.168.49.1 host.minikube.internal\n           fallthrough\n        }' -e '/^        errors *$/i \        log' | sudo /var/lib/minikube/binaries/v1.30.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig replace -f -"
I0621 22:09:31.586538  231799 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0621 22:09:31.683068  231799 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0621 22:09:31.683070  231799 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0621 22:09:32.096442  231799 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0621 22:09:32.096446  231799 start.go:946] {"host.minikube.internal": 192.168.49.1} host record injected into CoreDNS's ConfigMap
I0621 22:09:32.134797  231799 api_server.go:52] waiting for apiserver process to appear ...
I0621 22:09:32.134869  231799 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0621 22:09:32.322932  231799 api_server.go:72] duration metric: took 930.581127ms to wait for apiserver process to appear ...
I0621 22:09:32.322942  231799 api_server.go:88] waiting for apiserver healthz status ...
I0621 22:09:32.322954  231799 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:58062/healthz ...
I0621 22:09:32.328130  231799 api_server.go:279] https://127.0.0.1:58062/healthz returned 200:
ok
I0621 22:09:32.329605  231799 api_server.go:141] control plane version: v1.30.0
I0621 22:09:32.329618  231799 api_server.go:131] duration metric: took 6.672272ms to wait for apiserver health ...
I0621 22:09:32.331691  231799 out.go:177] üåü  Enabled addons: storage-provisioner, default-storageclass
I0621 22:09:32.329681  231799 system_pods.go:43] waiting for kube-system pods to appear ...
I0621 22:09:32.334834  231799 addons.go:505] duration metric: took 942.484212ms for enable addons: enabled=[storage-provisioner default-storageclass]
I0621 22:09:32.341911  231799 system_pods.go:59] 5 kube-system pods found
I0621 22:09:32.342019  231799 system_pods.go:61] "etcd-minikube" [083baac8-7539-4fab-b6e3-d4535ccdbf2a] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0621 22:09:32.342025  231799 system_pods.go:61] "kube-apiserver-minikube" [696cb180-c1bd-47d0-8ead-abc563ff5f36] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0621 22:09:32.342069  231799 system_pods.go:61] "kube-controller-manager-minikube" [552c06c5-2ab2-4ad6-8451-45042d3b2443] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0621 22:09:32.342075  231799 system_pods.go:61] "kube-scheduler-minikube" [66a32098-2b8d-430e-8937-4d1b8a278670] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0621 22:09:32.342077  231799 system_pods.go:61] "storage-provisioner" [fb455d40-05d4-4fe4-b7be-54a5edeea3dc] Pending
I0621 22:09:32.342082  231799 system_pods.go:74] duration metric: took 7.282797ms to wait for pod list to return data ...
I0621 22:09:32.342091  231799 kubeadm.go:576] duration metric: took 949.741208ms to wait for: map[apiserver:true system_pods:true]
I0621 22:09:32.342161  231799 node_conditions.go:102] verifying NodePressure condition ...
I0621 22:09:32.346867  231799 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I0621 22:09:32.346934  231799 node_conditions.go:123] node cpu capacity is 8
I0621 22:09:32.347185  231799 node_conditions.go:105] duration metric: took 4.959702ms to run NodePressure ...
I0621 22:09:32.347208  231799 start.go:240] waiting for startup goroutines ...
I0621 22:09:32.601310  231799 kapi.go:248] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I0621 22:09:32.601351  231799 start.go:245] waiting for cluster config update ...
I0621 22:09:32.601361  231799 start.go:254] writing updated cluster config ...
I0621 22:09:32.601770  231799 ssh_runner.go:195] Run: rm -f paused
I0621 22:09:32.737779  231799 start.go:600] kubectl: 1.30.2, cluster: 1.30.0 (minor skew: 0)
I0621 22:09:32.739890  231799 out.go:177] üèÑ  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Jun 21 19:09:17 minikube dockerd[1008]: time="2024-06-21T19:09:17.435808476Z" level=warning msg="WARNING: No blkio throttle.read_iops_device support"
Jun 21 19:09:17 minikube dockerd[1008]: time="2024-06-21T19:09:17.435818277Z" level=warning msg="WARNING: No blkio throttle.write_iops_device support"
Jun 21 19:09:17 minikube dockerd[1008]: time="2024-06-21T19:09:17.435845078Z" level=info msg="Docker daemon" commit=ac2de55 containerd-snapshotter=false storage-driver=overlay2 version=26.1.1
Jun 21 19:09:17 minikube dockerd[1008]: time="2024-06-21T19:09:17.435906980Z" level=info msg="Daemon has completed initialization"
Jun 21 19:09:17 minikube dockerd[1008]: time="2024-06-21T19:09:17.486462876Z" level=info msg="API listen on [::]:2376"
Jun 21 19:09:17 minikube dockerd[1008]: time="2024-06-21T19:09:17.486489277Z" level=info msg="API listen on /var/run/docker.sock"
Jun 21 19:09:17 minikube systemd[1]: Started Docker Application Container Engine.
Jun 21 19:09:17 minikube systemd[1]: Stopping Docker Application Container Engine...
Jun 21 19:09:17 minikube dockerd[1008]: time="2024-06-21T19:09:17.641943822Z" level=info msg="Processing signal 'terminated'"
Jun 21 19:09:17 minikube dockerd[1008]: time="2024-06-21T19:09:17.643838500Z" level=info msg="stopping event stream following graceful shutdown" error="<nil>" module=libcontainerd namespace=moby
Jun 21 19:09:17 minikube dockerd[1008]: time="2024-06-21T19:09:17.645148354Z" level=info msg="Daemon shutdown complete"
Jun 21 19:09:17 minikube systemd[1]: docker.service: Deactivated successfully.
Jun 21 19:09:17 minikube systemd[1]: Stopped Docker Application Container Engine.
Jun 21 19:09:17 minikube systemd[1]: Starting Docker Application Container Engine...
Jun 21 19:09:17 minikube dockerd[1229]: time="2024-06-21T19:09:17.710707372Z" level=info msg="Starting up"
Jun 21 19:09:17 minikube dockerd[1229]: time="2024-06-21T19:09:17.736235130Z" level=info msg="[graphdriver] trying configured driver: overlay2"
Jun 21 19:09:17 minikube dockerd[1229]: time="2024-06-21T19:09:17.753521047Z" level=info msg="Loading containers: start."
Jun 21 19:09:17 minikube dockerd[1229]: time="2024-06-21T19:09:17.891211355Z" level=info msg="Default bridge (docker0) is assigned with an IP address 172.17.0.0/16. Daemon option --bip can be used to set a preferred IP address"
Jun 21 19:09:17 minikube dockerd[1229]: time="2024-06-21T19:09:17.956736971Z" level=info msg="Loading containers: done."
Jun 21 19:09:17 minikube dockerd[1229]: time="2024-06-21T19:09:17.972905142Z" level=warning msg="WARNING: No blkio throttle.read_bps_device support"
Jun 21 19:09:17 minikube dockerd[1229]: time="2024-06-21T19:09:17.973006946Z" level=warning msg="WARNING: No blkio throttle.write_bps_device support"
Jun 21 19:09:17 minikube dockerd[1229]: time="2024-06-21T19:09:17.973040647Z" level=warning msg="WARNING: No blkio throttle.read_iops_device support"
Jun 21 19:09:17 minikube dockerd[1229]: time="2024-06-21T19:09:17.973050948Z" level=warning msg="WARNING: No blkio throttle.write_iops_device support"
Jun 21 19:09:17 minikube dockerd[1229]: time="2024-06-21T19:09:17.973081749Z" level=info msg="Docker daemon" commit=ac2de55 containerd-snapshotter=false storage-driver=overlay2 version=26.1.1
Jun 21 19:09:17 minikube dockerd[1229]: time="2024-06-21T19:09:17.973155052Z" level=info msg="Daemon has completed initialization"
Jun 21 19:09:18 minikube dockerd[1229]: time="2024-06-21T19:09:18.024620886Z" level=info msg="API listen on /var/run/docker.sock"
Jun 21 19:09:18 minikube dockerd[1229]: time="2024-06-21T19:09:18.024761391Z" level=info msg="API listen on [::]:2376"
Jun 21 19:09:18 minikube systemd[1]: Started Docker Application Container Engine.
Jun 21 19:09:18 minikube systemd[1]: Starting CRI Interface for Docker Application Container Engine...
Jun 21 19:09:18 minikube cri-dockerd[1455]: time="2024-06-21T19:09:18Z" level=info msg="Starting cri-dockerd dev (HEAD)"
Jun 21 19:09:18 minikube cri-dockerd[1455]: time="2024-06-21T19:09:18Z" level=info msg="Connecting to docker on the Endpoint unix:///var/run/docker.sock"
Jun 21 19:09:18 minikube cri-dockerd[1455]: time="2024-06-21T19:09:18Z" level=info msg="Start docker client with request timeout 0s"
Jun 21 19:09:18 minikube cri-dockerd[1455]: time="2024-06-21T19:09:18Z" level=info msg="Hairpin mode is set to hairpin-veth"
Jun 21 19:09:18 minikube cri-dockerd[1455]: time="2024-06-21T19:09:18Z" level=info msg="Loaded network plugin cni"
Jun 21 19:09:18 minikube cri-dockerd[1455]: time="2024-06-21T19:09:18Z" level=info msg="Docker cri networking managed by network plugin cni"
Jun 21 19:09:18 minikube cri-dockerd[1455]: time="2024-06-21T19:09:18Z" level=info msg="Setting cgroupDriver cgroupfs"
Jun 21 19:09:18 minikube cri-dockerd[1455]: time="2024-06-21T19:09:18Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:,},}"
Jun 21 19:09:18 minikube cri-dockerd[1455]: time="2024-06-21T19:09:18Z" level=info msg="Starting the GRPC backend for the Docker CRI interface."
Jun 21 19:09:18 minikube cri-dockerd[1455]: time="2024-06-21T19:09:18Z" level=info msg="Start cri-dockerd grpc backend"
Jun 21 19:09:18 minikube systemd[1]: Started CRI Interface for Docker Application Container Engine.
Jun 21 19:09:24 minikube cri-dockerd[1455]: time="2024-06-21T19:09:24Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/4a12fa03dbf2b315e6a8f3d63f8c78a882ea55bc96ca3b868613973c20370011/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Jun 21 19:09:24 minikube cri-dockerd[1455]: time="2024-06-21T19:09:24Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/6b1d488edb66dbb5369003fe4b7128e33f2d7bb11c3e094ad717c5de02cd02c6/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Jun 21 19:09:24 minikube cri-dockerd[1455]: time="2024-06-21T19:09:24Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/66dcffd2e899139325a2f2ca5c87baae3d9388775b8c685750f1e141657f42a4/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Jun 21 19:09:24 minikube cri-dockerd[1455]: time="2024-06-21T19:09:24Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/210b275512fb2997f06ffad978ca7c59d5969e0b83cf7d86af89249d166999ea/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Jun 21 19:09:45 minikube cri-dockerd[1455]: time="2024-06-21T19:09:45Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/d573970d5c0a7774c5db1a1004b5ab7966524bf53466b83dc9ad6b368f685a7f/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Jun 21 19:09:45 minikube cri-dockerd[1455]: time="2024-06-21T19:09:45Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/78ef3ca05dfb86bad390af10c6ec36882c289172bf7d370a93f5bd0b9521ae72/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Jun 21 19:09:46 minikube cri-dockerd[1455]: time="2024-06-21T19:09:46Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/067539294b92033b80e87b7873d4e01579a6d6c20301ed52bee50b808aba81c6/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Jun 21 19:09:51 minikube cri-dockerd[1455]: time="2024-06-21T19:09:51Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"
Jun 21 19:10:06 minikube dockerd[1229]: time="2024-06-21T19:10:06.485683827Z" level=info msg="ignoring event" container=733f56888f32db55fbff41185c424382fc30d789b00c19aa6ed8de5d82a023e9 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jun 21 19:10:31 minikube cri-dockerd[1455]: time="2024-06-21T19:10:31Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/96af0e3abf6a8cf147e59f5902a40ae21821e544b8d3d70a785bee86dccdfdff/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jun 21 19:10:43 minikube cri-dockerd[1455]: time="2024-06-21T19:10:43Z" level=info msg="Pulling image berkakipek96/go-rest-api:1.0: 5651b5803b18: Downloading [==========>                                        ]  5.167MB/24.05MB"
Jun 21 19:10:53 minikube cri-dockerd[1455]: time="2024-06-21T19:10:53Z" level=info msg="Pulling image berkakipek96/go-rest-api:1.0: fea1432adf09: Downloading [====================>                              ]  20.25MB/49.58MB"
Jun 21 19:11:03 minikube cri-dockerd[1455]: time="2024-06-21T19:11:03Z" level=info msg="Pulling image berkakipek96/go-rest-api:1.0: 5651b5803b18: Downloading [=================================>                 ]  16.24MB/24.05MB"
Jun 21 19:11:13 minikube cri-dockerd[1455]: time="2024-06-21T19:11:13Z" level=info msg="Pulling image berkakipek96/go-rest-api:1.0: fea1432adf09: Downloading [=======================================>           ]  39.01MB/49.58MB"
Jun 21 19:11:23 minikube cri-dockerd[1455]: time="2024-06-21T19:11:23Z" level=info msg="Pulling image berkakipek96/go-rest-api:1.0: fea1432adf09: Extracting [=====>                                             ]  5.767MB/49.58MB"
Jun 21 19:11:33 minikube cri-dockerd[1455]: time="2024-06-21T19:11:33Z" level=info msg="Pulling image berkakipek96/go-rest-api:1.0: 060aaf7efd06: Downloading [=======>                                           ]  10.23MB/69.35MB"
Jun 21 19:11:43 minikube cri-dockerd[1455]: time="2024-06-21T19:11:43Z" level=info msg="Pulling image berkakipek96/go-rest-api:1.0: 3873416e6a33: Extracting [======================================>            ]  49.02MB/64.14MB"
Jun 21 19:11:53 minikube cri-dockerd[1455]: time="2024-06-21T19:11:53Z" level=info msg="Pulling image berkakipek96/go-rest-api:1.0: 060aaf7efd06: Downloading [========================>                          ]  33.37MB/69.35MB"
Jun 21 19:12:03 minikube cri-dockerd[1455]: time="2024-06-21T19:12:03Z" level=info msg="Pulling image berkakipek96/go-rest-api:1.0: 3e81cd9a510c: Downloading [=========================>                         ]  46.21MB/92.2MB"
Jun 21 19:12:13 minikube cri-dockerd[1455]: time="2024-06-21T19:12:13Z" level=info msg="Pulling image berkakipek96/go-rest-api:1.0: 060aaf7efd06: Downloading [====================================>              ]  50.08MB/69.35MB"


==> container status <==
CONTAINER           IMAGE               CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
707db7b96adf7       6e38f40d628db       2 minutes ago       Running             storage-provisioner       1                   d573970d5c0a7       storage-provisioner
d07b0d02fede7       cbb01a7bd410d       2 minutes ago       Running             coredns                   0                   067539294b920       coredns-7db6d8ff4d-bmgcs
cc3947bff0647       a0bf559e280cf       2 minutes ago       Running             kube-proxy                0                   78ef3ca05dfb8       kube-proxy-pz48x
733f56888f32d       6e38f40d628db       2 minutes ago       Exited              storage-provisioner       0                   d573970d5c0a7       storage-provisioner
9f20fa03795e0       c42f13656d0b2       2 minutes ago       Running             kube-apiserver            0                   210b275512fb2       kube-apiserver-minikube
06d64659895d8       3861cfcd7c04c       2 minutes ago       Running             etcd                      0                   66dcffd2e8991       etcd-minikube
ff88a0f2df997       259c8277fcbbc       2 minutes ago       Running             kube-scheduler            0                   6b1d488edb66d       kube-scheduler-minikube
2c601c9e4062c       c7aad43836fa5       2 minutes ago       Running             kube-controller-manager   0                   4a12fa03dbf2b       kube-controller-manager-minikube


==> coredns [d07b0d02fede] <==
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 05e3eaddc414b2d71a69b2e2bc6f2681fc1f4d04bcdd3acc1a41457bb7db518208b95ddfc4c9fffedc59c25a8faf458be1af4915a4a3c0d6777cb7a346bc5d86
CoreDNS-1.11.1
linux/amd64, go1.20.7, ae2bbc2
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] 127.0.0.1:52019 - 11575 "HINFO IN 1369172457879679978.4704131622860975485. udp 57 false 512" - - 0 6.003690479s
[ERROR] plugin/errors: 2 1369172457879679978.4704131622860975485. HINFO: read udp 10.244.0.2:57948->192.168.65.254:53: i/o timeout
[INFO] 127.0.0.1:53206 - 13650 "HINFO IN 1369172457879679978.4704131622860975485. udp 57 false 512" - - 0 6.002503194s
[ERROR] plugin/errors: 2 1369172457879679978.4704131622860975485. HINFO: read udp 10.244.0.2:55117->192.168.65.254:53: i/o timeout
[INFO] 127.0.0.1:57618 - 7713 "HINFO IN 1369172457879679978.4704131622860975485. udp 57 false 512" - - 0 4.001028735s
[ERROR] plugin/errors: 2 1369172457879679978.4704131622860975485. HINFO: read udp 10.244.0.2:59625->192.168.65.254:53: i/o timeout
[INFO] 127.0.0.1:36428 - 22869 "HINFO IN 1369172457879679978.4704131622860975485. udp 57 false 512" - - 0 2.000705475s
[ERROR] plugin/errors: 2 1369172457879679978.4704131622860975485. HINFO: read udp 10.244.0.2:35499->192.168.65.254:53: i/o timeout
[INFO] 127.0.0.1:54897 - 25452 "HINFO IN 1369172457879679978.4704131622860975485. udp 57 false 512" - - 0 2.00053982s
[ERROR] plugin/errors: 2 1369172457879679978.4704131622860975485. HINFO: read udp 10.244.0.2:40325->192.168.65.254:53: i/o timeout
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: Trace[240664505]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (21-Jun-2024 19:09:46.393) (total time: 21047ms):
Trace[240664505]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused 21046ms (19:10:07.438)
Trace[240664505]: [21.04715206s] [21.04715206s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: Trace[1071295162]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (21-Jun-2024 19:09:46.393) (total time: 21047ms):
Trace[1071295162]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused 21046ms (19:10:07.438)
Trace[1071295162]: [21.047275666s] [21.047275666s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: Trace[732784833]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (21-Jun-2024 19:09:46.393) (total time: 21047ms):
Trace[732784833]: ---"Objects listed" error:Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused 21047ms (19:10:07.439)
Trace[732784833]: [21.047713384s] [21.047713384s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] 127.0.0.1:44155 - 38598 "HINFO IN 1369172457879679978.4704131622860975485. udp 57 false 512" - - 0 2.00200368s
[ERROR] plugin/errors: 2 1369172457879679978.4704131622860975485. HINFO: read udp 10.244.0.2:33792->192.168.65.254:53: i/o timeout
[INFO] 127.0.0.1:43500 - 7297 "HINFO IN 1369172457879679978.4704131622860975485. udp 57 false 512" - - 0 2.00052454s
[ERROR] plugin/errors: 2 1369172457879679978.4704131622860975485. HINFO: read udp 10.244.0.2:32953->192.168.65.254:53: i/o timeout
[INFO] 127.0.0.1:35980 - 31566 "HINFO IN 1369172457879679978.4704131622860975485. udp 57 false 512" - - 0 2.000995153s
[ERROR] plugin/errors: 2 1369172457879679978.4704131622860975485. HINFO: read udp 10.244.0.2:33542->192.168.65.254:53: i/o timeout
[INFO] 127.0.0.1:57004 - 60792 "HINFO IN 1369172457879679978.4704131622860975485. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.003304038s


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=5883c09216182566a63dff4c326a6fc9ed2982ff
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2024_06_21T22_09_31_0700
                    minikube.k8s.io/version=v1.33.1
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Fri, 21 Jun 2024 19:09:28 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Fri, 21 Jun 2024 19:12:14 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Fri, 21 Jun 2024 19:09:51 +0000   Fri, 21 Jun 2024 19:09:25 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Fri, 21 Jun 2024 19:09:51 +0000   Fri, 21 Jun 2024 19:09:25 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Fri, 21 Jun 2024 19:09:51 +0000   Fri, 21 Jun 2024 19:09:25 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Fri, 21 Jun 2024 19:09:51 +0000   Fri, 21 Jun 2024 19:09:30 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                8
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             8075300Ki
  pods:               110
Allocatable:
  cpu:                8
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             8075300Ki
  pods:               110
System Info:
  Machine ID:                 276d6364fba447c99deec8319a5b9875
  System UUID:                276d6364fba447c99deec8319a5b9875
  Boot ID:                    59428c36-4844-435c-a585-8156d44f084b
  Kernel Version:             5.15.153.1-microsoft-standard-WSL2
  OS Image:                   Ubuntu 22.04.4 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://26.1.1
  Kubelet Version:            v1.30.0
  Kube-Proxy Version:         v1.30.0
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (8 in total)
  Namespace                   Name                                CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                ------------  ----------  ---------------  -------------  ---
  default                     go-rest-api-f97dd8f58-9zql6         500m (6%!)(MISSING)     1 (12%!)(MISSING)     256Mi (3%!)(MISSING)       512Mi (6%!)(MISSING)     114s
  kube-system                 coredns-7db6d8ff4d-bmgcs            100m (1%!)(MISSING)     0 (0%!)(MISSING)      70Mi (0%!)(MISSING)        170Mi (2%!)(MISSING)     2m35s
  kube-system                 etcd-minikube                       100m (1%!)(MISSING)     0 (0%!)(MISSING)      100Mi (1%!)(MISSING)       0 (0%!)(MISSING)         2m50s
  kube-system                 kube-apiserver-minikube             250m (3%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         2m50s
  kube-system                 kube-controller-manager-minikube    200m (2%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         2m50s
  kube-system                 kube-proxy-pz48x                    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         2m35s
  kube-system                 kube-scheduler-minikube             100m (1%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         2m50s
  kube-system                 storage-provisioner                 0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         2m48s
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests     Limits
  --------           --------     ------
  cpu                1250m (15%!)(MISSING)  1 (12%!)(MISSING)
  memory             426Mi (5%!)(MISSING)   682Mi (8%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)       0 (0%!)(MISSING)
  hugepages-1Gi      0 (0%!)(MISSING)       0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)       0 (0%!)(MISSING)
Events:
  Type    Reason                   Age                    From             Message
  ----    ------                   ----                   ----             -------
  Normal  Starting                 2m34s                  kube-proxy       
  Normal  NodeHasSufficientMemory  2m56s (x8 over 2m56s)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    2m56s (x8 over 2m56s)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     2m56s (x7 over 2m56s)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  NodeAllocatableEnforced  2m56s                  kubelet          Updated Node Allocatable limit across pods
  Normal  NodeHasSufficientMemory  2m50s                  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    2m50s                  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     2m50s                  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  NodeNotReady             2m50s                  kubelet          Node minikube status is now: NodeNotReady
  Normal  NodeAllocatableEnforced  2m50s                  kubelet          Updated Node Allocatable limit across pods
  Normal  NodeReady                2m50s                  kubelet          Node minikube status is now: NodeReady
  Normal  Starting                 2m50s                  kubelet          Starting kubelet.
  Normal  RegisteredNode           2m36s                  node-controller  Node minikube event: Registered Node minikube in Controller


==> dmesg <==
[  +0.000546] FS-Cache: O-key=[10] '34323934393337333830'
[  +0.000355] FS-Cache: N-cookie c=00000007 [p=00000002 fl=2 nc=0 na=1]
[  +0.000451] FS-Cache: N-cookie d=00000000ff0a6224{9P.session} n=00000000d33e68cd
[  +0.000875] FS-Cache: N-key=[10] '34323934393337333830'
[  +0.513622] /sbin/ldconfig: 
[  +0.000004] /usr/lib/wsl/lib/libcuda.so.1 is not a symbolic link

[  +0.169141] WSL (1) ERROR: ConfigApplyWindowsLibPath:2537: open /etc/ld.so.conf.d/ld.wsl.conf
[  +0.000006]  failed 2
[  +0.017617] WSL (1) WARNING: /usr/share/zoneinfo/Europe/Istanbul not found. Is the tzdata package installed?
[  +0.320449] misc dxg: dxgk: dxgkio_is_feature_enabled: Ioctl failed: -22
[  +0.009887] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000819] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000813] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000962] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.002037] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000916] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000734] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000903] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.383977] /sbin/ldconfig: 
[  +0.000004] /usr/lib/wsl/lib/libcuda.so.1 is not a symbolic link

[  +0.028411] WSL (2) ERROR: UtilCreateProcessAndWait:665: /bin/mount failed with 2
[  +0.001138] WSL (1) ERROR: UtilCreateProcessAndWait:687: /bin/mount failed with status 0xff00

[  +0.001177] WSL (1) ERROR: ConfigMountFsTab:2589: Processing fstab with mount -a failed.
[  +0.007228] WSL (1) ERROR: ConfigApplyWindowsLibPath:2537: open /etc/ld.so.conf.d/ld.wsl.conf
[  +0.000005]  failed 2
[  +0.003619] WSL (3) ERROR: UtilCreateProcessAndWait:665: /bin/mount failed with 2
[  +0.001075] WSL (1) ERROR: UtilCreateProcessAndWait:687: /bin/mount failed with status 0xff00

[  +0.055618] WSL (1) WARNING: /usr/share/zoneinfo/Europe/Istanbul not found. Is the tzdata package installed?
[  +0.101239] misc dxg: dxgk: dxgkio_is_feature_enabled: Ioctl failed: -22
[  +0.004078] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001857] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001267] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001655] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.002787] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001274] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001137] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001138] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.504249] /sbin/ldconfig: 
[  +0.000005] /usr/lib/wsl/lib/libcuda.so.1 is not a symbolic link

[  +0.144944] /sbin/ldconfig.real: 
[  +0.000005] /usr/lib/wsl/lib/libcuda.so.1 is not a symbolic link

[  +0.032685] netlink: 'init': attribute type 4 has an invalid length.
[  +0.052469] misc dxg: dxgk: dxgkio_is_feature_enabled: Ioctl failed: -22
[  +0.002419] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001426] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000641] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001627] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.002469] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.003057] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001151] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001412] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.549869] new mount options do not match the existing superblock, will be ignored
[  +0.243159] Failed to connect to bus: No such file or directory
[  +0.235252] systemd-journald[38]: File /var/log/journal/1addef89b27b495c86e5a2bf56baa36f/system.journal corrupted or uncleanly shut down, renaming and replacing.


==> etcd [06d64659895d] <==
{"level":"warn","ts":"2024-06-21T19:09:25.389177Z","caller":"embed/config.go:679","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2024-06-21T19:09:25.389315Z","caller":"etcdmain/etcd.go:73","msg":"Running: ","args":["etcd","--advertise-client-urls=https://192.168.49.2:2379","--cert-file=/var/lib/minikube/certs/etcd/server.crt","--client-cert-auth=true","--data-dir=/var/lib/minikube/etcd","--experimental-initial-corrupt-check=true","--experimental-watch-progress-notify-interval=5s","--initial-advertise-peer-urls=https://192.168.49.2:2380","--initial-cluster=minikube=https://192.168.49.2:2380","--key-file=/var/lib/minikube/certs/etcd/server.key","--listen-client-urls=https://127.0.0.1:2379,https://192.168.49.2:2379","--listen-metrics-urls=http://127.0.0.1:2381","--listen-peer-urls=https://192.168.49.2:2380","--name=minikube","--peer-cert-file=/var/lib/minikube/certs/etcd/peer.crt","--peer-client-cert-auth=true","--peer-key-file=/var/lib/minikube/certs/etcd/peer.key","--peer-trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt","--proxy-refresh-interval=70000","--snapshot-count=10000","--trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt"]}
{"level":"warn","ts":"2024-06-21T19:09:25.38943Z","caller":"embed/config.go:679","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2024-06-21T19:09:25.389448Z","caller":"embed/etcd.go:127","msg":"configuring peer listeners","listen-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2024-06-21T19:09:25.389495Z","caller":"embed/etcd.go:494","msg":"starting with peer TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/peer.crt, key = /var/lib/minikube/certs/etcd/peer.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2024-06-21T19:09:25.390285Z","caller":"embed/etcd.go:135","msg":"configuring client listeners","listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"]}
{"level":"info","ts":"2024-06-21T19:09:25.390423Z","caller":"embed/etcd.go:308","msg":"starting an etcd server","etcd-version":"3.5.12","git-sha":"e7b3bb6cc","go-version":"go1.20.13","go-os":"linux","go-arch":"amd64","max-cpu-set":8,"max-cpu-available":8,"member-initialized":false,"name":"minikube","data-dir":"/var/lib/minikube/etcd","wal-dir":"","wal-dir-dedicated":"","member-dir":"/var/lib/minikube/etcd/member","force-new-cluster":false,"heartbeat-interval":"100ms","election-timeout":"1s","initial-election-tick-advance":true,"snapshot-count":10000,"max-wals":5,"max-snapshots":5,"snapshot-catchup-entries":5000,"initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"],"cors":["*"],"host-whitelist":["*"],"initial-cluster":"minikube=https://192.168.49.2:2380","initial-cluster-state":"new","initial-cluster-token":"etcd-cluster","quota-backend-bytes":2147483648,"max-request-bytes":1572864,"max-concurrent-streams":4294967295,"pre-vote":true,"initial-corrupt-check":true,"corrupt-check-time-interval":"0s","compact-check-time-enabled":false,"compact-check-time-interval":"1m0s","auto-compaction-mode":"periodic","auto-compaction-retention":"0s","auto-compaction-interval":"0s","discovery-url":"","discovery-proxy":"","downgrade-check-interval":"5s"}
{"level":"info","ts":"2024-06-21T19:09:25.455751Z","caller":"etcdserver/backend.go:81","msg":"opened backend db","path":"/var/lib/minikube/etcd/member/snap/db","took":"64.203907ms"}
{"level":"info","ts":"2024-06-21T19:09:25.463243Z","caller":"etcdserver/raft.go:495","msg":"starting local member","local-member-id":"aec36adc501070cc","cluster-id":"fa54960ea34d58be"}
{"level":"info","ts":"2024-06-21T19:09:25.463379Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=()"}
{"level":"info","ts":"2024-06-21T19:09:25.46343Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 0"}
{"level":"info","ts":"2024-06-21T19:09:25.463478Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft aec36adc501070cc [peers: [], term: 0, commit: 0, applied: 0, lastindex: 0, lastterm: 0]"}
{"level":"info","ts":"2024-06-21T19:09:25.4635Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 1"}
{"level":"info","ts":"2024-06-21T19:09:25.463559Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"warn","ts":"2024-06-21T19:09:25.470533Z","caller":"auth/store.go:1241","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2024-06-21T19:09:25.473575Z","caller":"mvcc/kvstore.go:407","msg":"kvstore restored","current-rev":1}
{"level":"info","ts":"2024-06-21T19:09:25.476139Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2024-06-21T19:09:25.480531Z","caller":"etcdserver/server.go:860","msg":"starting etcd server","local-member-id":"aec36adc501070cc","local-server-version":"3.5.12","cluster-version":"to_be_decided"}
{"level":"info","ts":"2024-06-21T19:09:25.480781Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2024-06-21T19:09:25.480858Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2024-06-21T19:09:25.480873Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2024-06-21T19:09:25.480969Z","caller":"etcdserver/server.go:744","msg":"started as single-node; fast-forwarding election ticks","local-member-id":"aec36adc501070cc","forward-ticks":9,"forward-duration":"900ms","election-ticks":10,"election-timeout":"1s"}
{"level":"info","ts":"2024-06-21T19:09:25.482805Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"info","ts":"2024-06-21T19:09:25.48605Z","caller":"membership/cluster.go:421","msg":"added member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","added-peer-id":"aec36adc501070cc","added-peer-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2024-06-21T19:09:25.495499Z","caller":"embed/etcd.go:726","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2024-06-21T19:09:25.495685Z","caller":"embed/etcd.go:277","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2024-06-21T19:09:25.49572Z","caller":"embed/etcd.go:857","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2024-06-21T19:09:25.495804Z","caller":"embed/etcd.go:597","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-06-21T19:09:25.495826Z","caller":"embed/etcd.go:569","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-06-21T19:09:26.264705Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 1"}
{"level":"info","ts":"2024-06-21T19:09:26.264748Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 1"}
{"level":"info","ts":"2024-06-21T19:09:26.264789Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 1"}
{"level":"info","ts":"2024-06-21T19:09:26.264822Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 2"}
{"level":"info","ts":"2024-06-21T19:09:26.264839Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 2"}
{"level":"info","ts":"2024-06-21T19:09:26.264891Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 2"}
{"level":"info","ts":"2024-06-21T19:09:26.264905Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 2"}
{"level":"info","ts":"2024-06-21T19:09:26.26635Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-06-21T19:09:26.266384Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-06-21T19:09:26.266339Z","caller":"etcdserver/server.go:2068","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2024-06-21T19:09:26.266348Z","caller":"etcdserver/server.go:2578","msg":"setting up initial cluster version using v2 API","cluster-version":"3.5"}
{"level":"info","ts":"2024-06-21T19:09:26.266594Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2024-06-21T19:09:26.266979Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2024-06-21T19:09:26.26944Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"info","ts":"2024-06-21T19:09:26.269502Z","caller":"membership/cluster.go:584","msg":"set initial cluster version","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","cluster-version":"3.5"}
{"level":"info","ts":"2024-06-21T19:09:26.26952Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.49.2:2379"}
{"level":"info","ts":"2024-06-21T19:09:26.269803Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2024-06-21T19:09:26.269941Z","caller":"etcdserver/server.go:2602","msg":"cluster version is updated","cluster-version":"3.5"}


==> kernel <==
 19:12:20 up  7:46,  0 users,  load average: 1.31, 1.08, 0.92
Linux minikube 5.15.153.1-microsoft-standard-WSL2 #1 SMP Fri Mar 29 23:14:13 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.4 LTS"


==> kube-apiserver [9f20fa03795e] <==
I0621 19:09:27.892694       1 aggregator.go:163] waiting for initial CRD sync...
I0621 19:09:27.892760       1 controller.go:116] Starting legacy_token_tracking_controller
I0621 19:09:27.892821       1 shared_informer.go:313] Waiting for caches to sync for configmaps
I0621 19:09:27.893163       1 cluster_authentication_trust_controller.go:440] Starting cluster_authentication_trust_controller controller
I0621 19:09:27.893180       1 shared_informer.go:313] Waiting for caches to sync for cluster_authentication_trust_controller
I0621 19:09:27.893211       1 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0621 19:09:27.893218       1 apiservice_controller.go:97] Starting APIServiceRegistrationController
I0621 19:09:27.893231       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I0621 19:09:27.893259       1 controller.go:80] Starting OpenAPI V3 AggregationController
I0621 19:09:27.893279       1 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0621 19:09:27.893288       1 controller.go:78] Starting OpenAPI AggregationController
I0621 19:09:27.893318       1 apf_controller.go:374] Starting API Priority and Fairness config controller
I0621 19:09:27.893475       1 dynamic_serving_content.go:132] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I0621 19:09:27.893610       1 available_controller.go:423] Starting AvailableConditionController
I0621 19:09:27.893619       1 cache.go:32] Waiting for caches to sync for AvailableConditionController controller
I0621 19:09:27.893750       1 customresource_discovery_controller.go:289] Starting DiscoveryController
I0621 19:09:27.893793       1 gc_controller.go:78] Starting apiserver lease garbage collector
I0621 19:09:27.893825       1 crdregistration_controller.go:111] Starting crd-autoregister controller
I0621 19:09:27.893831       1 shared_informer.go:313] Waiting for caches to sync for crd-autoregister
I0621 19:09:27.895308       1 controller.go:139] Starting OpenAPI controller
I0621 19:09:27.895433       1 controller.go:87] Starting OpenAPI V3 controller
I0621 19:09:27.895512       1 naming_controller.go:291] Starting NamingConditionController
I0621 19:09:27.895645       1 establishing_controller.go:76] Starting EstablishingController
I0621 19:09:27.895753       1 nonstructuralschema_controller.go:192] Starting NonStructuralSchemaConditionController
I0621 19:09:27.895919       1 apiapproval_controller.go:186] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0621 19:09:27.896078       1 crd_finalizer.go:266] Starting CRDFinalizer
I0621 19:09:28.055347       1 shared_informer.go:320] Caches are synced for configmaps
I0621 19:09:28.055371       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0621 19:09:28.055406       1 apf_controller.go:379] Running API Priority and Fairness config worker
I0621 19:09:28.055419       1 apf_controller.go:382] Running API Priority and Fairness periodic rebalancing process
I0621 19:09:28.055494       1 shared_informer.go:320] Caches are synced for node_authorizer
I0621 19:09:28.055557       1 cache.go:39] Caches are synced for AvailableConditionController controller
I0621 19:09:28.055771       1 shared_informer.go:320] Caches are synced for crd-autoregister
I0621 19:09:28.056035       1 handler_discovery.go:447] Starting ResourceDiscoveryManager
I0621 19:09:28.056080       1 aggregator.go:165] initial CRD sync complete...
I0621 19:09:28.056090       1 autoregister_controller.go:141] Starting autoregister controller
I0621 19:09:28.056097       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0621 19:09:28.056106       1 cache.go:39] Caches are synced for autoregister controller
I0621 19:09:28.056289       1 shared_informer.go:320] Caches are synced for cluster_authentication_trust_controller
I0621 19:09:28.056325       1 shared_informer.go:320] Caches are synced for *generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]
I0621 19:09:28.056335       1 policy_source.go:224] refreshing policies
I0621 19:09:28.056810       1 controller.go:615] quota admission added evaluator for: namespaces
E0621 19:09:28.073799       1 controller.go:145] "Failed to ensure lease exists, will retry" err="namespaces \"kube-system\" not found" interval="200ms"
I0621 19:09:28.277436       1 controller.go:615] quota admission added evaluator for: leases.coordination.k8s.io
I0621 19:09:28.907232       1 storage_scheduling.go:95] created PriorityClass system-node-critical with value 2000001000
I0621 19:09:28.913019       1 storage_scheduling.go:95] created PriorityClass system-cluster-critical with value 2000000000
I0621 19:09:28.913054       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0621 19:09:29.518387       1 controller.go:615] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I0621 19:09:29.559887       1 controller.go:615] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I0621 19:09:29.689950       1 alloc.go:330] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.96.0.1"}
W0621 19:09:29.699134       1 lease.go:265] Resetting endpoints for master service "kubernetes" to [192.168.49.2]
I0621 19:09:29.700054       1 controller.go:615] quota admission added evaluator for: endpoints
I0621 19:09:29.708786       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0621 19:09:29.979736       1 controller.go:615] quota admission added evaluator for: serviceaccounts
I0621 19:09:30.662441       1 controller.go:615] quota admission added evaluator for: deployments.apps
I0621 19:09:30.684810       1 alloc.go:330] "allocated clusterIPs" service="kube-system/kube-dns" clusterIPs={"IPv4":"10.96.0.10"}
I0621 19:09:30.767649       1 controller.go:615] quota admission added evaluator for: daemonsets.apps
I0621 19:09:45.258484       1 controller.go:615] quota admission added evaluator for: replicasets.apps
I0621 19:09:45.306442       1 controller.go:615] quota admission added evaluator for: controllerrevisions.apps
I0621 19:10:26.683229       1 alloc.go:330] "allocated clusterIPs" service="default/go-rest-api-service" clusterIPs={"IPv4":"10.108.25.151"}


==> kube-controller-manager [2c601c9e4062] <==
I0621 19:09:44.256187       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kubelet-serving
I0621 19:09:44.256355       1 shared_informer.go:320] Caches are synced for crt configmap
I0621 19:09:44.257668       1 shared_informer.go:320] Caches are synced for TTL
I0621 19:09:44.259120       1 shared_informer.go:320] Caches are synced for service account
I0621 19:09:44.259139       1 shared_informer.go:320] Caches are synced for namespace
I0621 19:09:44.259881       1 shared_informer.go:320] Caches are synced for PV protection
I0621 19:09:44.260158       1 shared_informer.go:320] Caches are synced for ClusterRoleAggregator
I0621 19:09:44.261692       1 shared_informer.go:313] Waiting for caches to sync for garbage collector
I0621 19:09:44.304077       1 shared_informer.go:320] Caches are synced for endpoint_slice_mirroring
I0621 19:09:44.304097       1 shared_informer.go:320] Caches are synced for TTL after finished
I0621 19:09:44.305281       1 shared_informer.go:320] Caches are synced for bootstrap_signer
I0621 19:09:44.310964       1 shared_informer.go:320] Caches are synced for validatingadmissionpolicy-status
I0621 19:09:44.314416       1 shared_informer.go:320] Caches are synced for node
I0621 19:09:44.314502       1 range_allocator.go:175] "Sending events to api server" logger="node-ipam-controller"
I0621 19:09:44.314534       1 range_allocator.go:179] "Starting range CIDR allocator" logger="node-ipam-controller"
I0621 19:09:44.314548       1 shared_informer.go:313] Waiting for caches to sync for cidrallocator
I0621 19:09:44.314562       1 shared_informer.go:320] Caches are synced for cidrallocator
I0621 19:09:44.321265       1 range_allocator.go:381] "Set node PodCIDR" logger="node-ipam-controller" node="minikube" podCIDRs=["10.244.0.0/24"]
I0621 19:09:44.338217       1 shared_informer.go:320] Caches are synced for cronjob
I0621 19:09:44.343893       1 shared_informer.go:320] Caches are synced for certificate-csrapproving
I0621 19:09:44.439078       1 shared_informer.go:320] Caches are synced for endpoint_slice
I0621 19:09:44.451078       1 shared_informer.go:320] Caches are synced for legacy-service-account-token-cleaner
I0621 19:09:44.452327       1 shared_informer.go:320] Caches are synced for GC
I0621 19:09:44.457081       1 shared_informer.go:320] Caches are synced for endpoint
I0621 19:09:44.479648       1 shared_informer.go:320] Caches are synced for resource quota
I0621 19:09:44.484207       1 shared_informer.go:320] Caches are synced for deployment
I0621 19:09:44.489971       1 shared_informer.go:320] Caches are synced for daemon sets
I0621 19:09:44.502258       1 shared_informer.go:320] Caches are synced for ReplicaSet
I0621 19:09:44.503422       1 shared_informer.go:320] Caches are synced for taint
I0621 19:09:44.503502       1 node_lifecycle_controller.go:1227] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I0621 19:09:44.503571       1 shared_informer.go:320] Caches are synced for taint-eviction-controller
I0621 19:09:44.503579       1 node_lifecycle_controller.go:879] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I0621 19:09:44.503662       1 node_lifecycle_controller.go:1073] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I0621 19:09:44.504015       1 shared_informer.go:320] Caches are synced for persistent volume
I0621 19:09:44.504088       1 shared_informer.go:320] Caches are synced for HPA
I0621 19:09:44.505105       1 shared_informer.go:320] Caches are synced for ReplicationController
I0621 19:09:44.505105       1 shared_informer.go:320] Caches are synced for job
I0621 19:09:44.506365       1 shared_informer.go:320] Caches are synced for PVC protection
I0621 19:09:44.508406       1 shared_informer.go:320] Caches are synced for attach detach
I0621 19:09:44.515561       1 shared_informer.go:320] Caches are synced for disruption
I0621 19:09:44.522961       1 shared_informer.go:320] Caches are synced for stateful set
I0621 19:09:44.526017       1 shared_informer.go:320] Caches are synced for resource quota
I0621 19:09:44.528000       1 shared_informer.go:320] Caches are synced for ephemeral
I0621 19:09:44.554507       1 shared_informer.go:320] Caches are synced for expand
I0621 19:09:44.961935       1 shared_informer.go:320] Caches are synced for garbage collector
I0621 19:09:45.003016       1 shared_informer.go:320] Caches are synced for garbage collector
I0621 19:09:45.003061       1 garbagecollector.go:157] "All resource monitors have synced. Proceeding to collect garbage" logger="garbage-collector-controller"
I0621 19:09:45.420662       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-7db6d8ff4d" duration="157.499803ms"
I0621 19:09:45.459777       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-7db6d8ff4d" duration="39.068613ms"
I0621 19:09:45.459982       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-7db6d8ff4d" duration="112.605¬µs"
I0621 19:09:45.462874       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-7db6d8ff4d" duration="44.701¬µs"
I0621 19:09:47.112543       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-7db6d8ff4d" duration="50.302¬µs"
I0621 19:10:15.784099       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-7db6d8ff4d" duration="12.745835ms"
I0621 19:10:15.784343       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-7db6d8ff4d" duration="59.702¬µs"
I0621 19:10:26.688553       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/go-rest-api-f97dd8f58" duration="25.776467ms"
I0621 19:10:26.699561       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/go-rest-api-f97dd8f58" duration="10.929652ms"
I0621 19:10:26.699913       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/go-rest-api-f97dd8f58" duration="36.901¬µs"
I0621 19:10:28.513997       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/go-rest-api-f97dd8f58" duration="66.602¬µs"
I0621 19:10:30.515855       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/go-rest-api-f97dd8f58" duration="228.31¬µs"
I0621 19:10:30.540809       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/go-rest-api-f97dd8f58" duration="63.003¬µs"


==> kube-proxy [cc3947bff064] <==
I0621 19:09:46.231338       1 server_linux.go:69] "Using iptables proxy"
I0621 19:09:46.260655       1 server.go:1062] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
I0621 19:09:46.289526       1 server.go:659] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0621 19:09:46.289579       1 server_linux.go:165] "Using iptables Proxier"
I0621 19:09:46.291855       1 server_linux.go:511] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I0621 19:09:46.291893       1 server_linux.go:528] "Defaulting to no-op detect-local"
I0621 19:09:46.291908       1 proxier.go:243] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0621 19:09:46.292130       1 server.go:872] "Version info" version="v1.30.0"
I0621 19:09:46.292173       1 server.go:874] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0621 19:09:46.293572       1 config.go:101] "Starting endpoint slice config controller"
I0621 19:09:46.293619       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0621 19:09:46.293627       1 config.go:319] "Starting node config controller"
I0621 19:09:46.293649       1 shared_informer.go:313] Waiting for caches to sync for node config
I0621 19:09:46.293908       1 config.go:192] "Starting service config controller"
I0621 19:09:46.293919       1 shared_informer.go:313] Waiting for caches to sync for service config
I0621 19:09:46.394189       1 shared_informer.go:320] Caches are synced for service config
I0621 19:09:46.394243       1 shared_informer.go:320] Caches are synced for node config
I0621 19:09:46.394201       1 shared_informer.go:320] Caches are synced for endpoint slice config


==> kube-scheduler [ff88a0f2df99] <==
I0621 19:09:28.059484       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0621 19:09:28.060509       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0621 19:09:28.060749       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
W0621 19:09:28.065376       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0621 19:09:28.065465       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
W0621 19:09:28.065669       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0621 19:09:28.065778       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
W0621 19:09:28.066854       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0621 19:09:28.066937       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
W0621 19:09:28.066882       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0621 19:09:28.067274       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
W0621 19:09:28.067664       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0621 19:09:28.067729       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W0621 19:09:28.068474       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0621 19:09:28.068585       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
W0621 19:09:28.068588       1 reflector.go:547] runtime/asm_amd64.s:1695: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0621 19:09:28.068657       1 reflector.go:150] runtime/asm_amd64.s:1695: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
W0621 19:09:28.069476       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0621 19:09:28.069529       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
W0621 19:09:28.069555       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0621 19:09:28.069603       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
W0621 19:09:28.069666       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E0621 19:09:28.069709       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
W0621 19:09:28.069766       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0621 19:09:28.070027       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
W0621 19:09:28.070209       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0621 19:09:28.072128       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
W0621 19:09:28.070210       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0621 19:09:28.072258       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
W0621 19:09:28.070773       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0621 19:09:28.072291       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
W0621 19:09:28.071057       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0621 19:09:28.072339       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
W0621 19:09:28.953528       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0621 19:09:28.953575       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
W0621 19:09:28.960584       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0621 19:09:28.960632       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W0621 19:09:29.066302       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E0621 19:09:29.066348       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
W0621 19:09:29.108601       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0621 19:09:29.108659       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
W0621 19:09:29.117971       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0621 19:09:29.118034       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
W0621 19:09:29.123355       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0621 19:09:29.123404       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
W0621 19:09:29.175447       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0621 19:09:29.175488       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
W0621 19:09:29.187588       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0621 19:09:29.187657       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
W0621 19:09:29.238143       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0621 19:09:29.238369       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
W0621 19:09:29.262949       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0621 19:09:29.263008       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
W0621 19:09:29.273485       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0621 19:09:29.273532       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
W0621 19:09:29.294175       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0621 19:09:29.294229       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
W0621 19:09:29.466708       1 reflector.go:547] runtime/asm_amd64.s:1695: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0621 19:09:29.466777       1 reflector.go:150] runtime/asm_amd64.s:1695: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
I0621 19:09:31.360583       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kubelet <==
Jun 21 19:09:30 minikube kubelet[2386]: I0621 19:09:30.858892    2386 cpu_manager.go:214] "Starting CPU manager" policy="none"
Jun 21 19:09:30 minikube kubelet[2386]: I0621 19:09:30.858928    2386 cpu_manager.go:215] "Reconciling" reconcilePeriod="10s"
Jun 21 19:09:30 minikube kubelet[2386]: I0621 19:09:30.858962    2386 state_mem.go:36] "Initialized new in-memory state store"
Jun 21 19:09:30 minikube kubelet[2386]: I0621 19:09:30.859169    2386 state_mem.go:88] "Updated default CPUSet" cpuSet=""
Jun 21 19:09:30 minikube kubelet[2386]: I0621 19:09:30.859213    2386 state_mem.go:96] "Updated CPUSet assignments" assignments={}
Jun 21 19:09:30 minikube kubelet[2386]: I0621 19:09:30.859251    2386 policy_none.go:49] "None policy: Start"
Jun 21 19:09:30 minikube kubelet[2386]: I0621 19:09:30.864034    2386 memory_manager.go:170] "Starting memorymanager" policy="None"
Jun 21 19:09:30 minikube kubelet[2386]: I0621 19:09:30.864128    2386 state_mem.go:35] "Initializing new in-memory state store"
Jun 21 19:09:30 minikube kubelet[2386]: I0621 19:09:30.864468    2386 state_mem.go:75] "Updated machine memory state"
Jun 21 19:09:30 minikube kubelet[2386]: I0621 19:09:30.866251    2386 manager.go:479] "Failed to read data from checkpoint" checkpoint="kubelet_internal_checkpoint" err="checkpoint is not found"
Jun 21 19:09:30 minikube kubelet[2386]: I0621 19:09:30.866443    2386 container_log_manager.go:186] "Initializing container log rotate workers" workers=1 monitorPeriod="10s"
Jun 21 19:09:30 minikube kubelet[2386]: I0621 19:09:30.866669    2386 plugin_manager.go:118] "Starting Kubelet Plugin Manager"
Jun 21 19:09:30 minikube kubelet[2386]: I0621 19:09:30.867990    2386 kubelet_node_status.go:497] "Fast updating node status as it just became ready"
Jun 21 19:09:30 minikube kubelet[2386]: I0621 19:09:30.970150    2386 topology_manager.go:215] "Topology Admit Handler" podUID="063d6b9688927e601f52fd818d1305c5" podNamespace="kube-system" podName="etcd-minikube"
Jun 21 19:09:30 minikube kubelet[2386]: I0621 19:09:30.970346    2386 topology_manager.go:215] "Topology Admit Handler" podUID="3c555f828409b009ebee39fdbedfcac0" podNamespace="kube-system" podName="kube-apiserver-minikube"
Jun 21 19:09:30 minikube kubelet[2386]: I0621 19:09:30.970420    2386 topology_manager.go:215] "Topology Admit Handler" podUID="7fd44e8d11c3e0ffe6b1825e2a1f2270" podNamespace="kube-system" podName="kube-controller-manager-minikube"
Jun 21 19:09:30 minikube kubelet[2386]: I0621 19:09:30.970460    2386 topology_manager.go:215] "Topology Admit Handler" podUID="f9c8e1d0d74b1727abdb4b4a31d3a7c1" podNamespace="kube-system" podName="kube-scheduler-minikube"
Jun 21 19:09:31 minikube kubelet[2386]: I0621 19:09:31.057378    2386 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"k8s-certs\" (UniqueName: \"kubernetes.io/host-path/3c555f828409b009ebee39fdbedfcac0-k8s-certs\") pod \"kube-apiserver-minikube\" (UID: \"3c555f828409b009ebee39fdbedfcac0\") " pod="kube-system/kube-apiserver-minikube"
Jun 21 19:09:31 minikube kubelet[2386]: I0621 19:09:31.057485    2386 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/3c555f828409b009ebee39fdbedfcac0-usr-share-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"3c555f828409b009ebee39fdbedfcac0\") " pod="kube-system/kube-apiserver-minikube"
Jun 21 19:09:31 minikube kubelet[2386]: I0621 19:09:31.057533    2386 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/7fd44e8d11c3e0ffe6b1825e2a1f2270-usr-share-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"7fd44e8d11c3e0ffe6b1825e2a1f2270\") " pod="kube-system/kube-controller-manager-minikube"
Jun 21 19:09:31 minikube kubelet[2386]: I0621 19:09:31.057556    2386 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"ca-certs\" (UniqueName: \"kubernetes.io/host-path/3c555f828409b009ebee39fdbedfcac0-ca-certs\") pod \"kube-apiserver-minikube\" (UID: \"3c555f828409b009ebee39fdbedfcac0\") " pod="kube-system/kube-apiserver-minikube"
Jun 21 19:09:31 minikube kubelet[2386]: I0621 19:09:31.057578    2386 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etc-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/3c555f828409b009ebee39fdbedfcac0-etc-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"3c555f828409b009ebee39fdbedfcac0\") " pod="kube-system/kube-apiserver-minikube"
Jun 21 19:09:31 minikube kubelet[2386]: I0621 19:09:31.057602    2386 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etcd-certs\" (UniqueName: \"kubernetes.io/host-path/063d6b9688927e601f52fd818d1305c5-etcd-certs\") pod \"etcd-minikube\" (UID: \"063d6b9688927e601f52fd818d1305c5\") " pod="kube-system/etcd-minikube"
Jun 21 19:09:31 minikube kubelet[2386]: I0621 19:09:31.057623    2386 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etcd-data\" (UniqueName: \"kubernetes.io/host-path/063d6b9688927e601f52fd818d1305c5-etcd-data\") pod \"etcd-minikube\" (UID: \"063d6b9688927e601f52fd818d1305c5\") " pod="kube-system/etcd-minikube"
Jun 21 19:09:31 minikube kubelet[2386]: I0621 19:09:31.057641    2386 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"ca-certs\" (UniqueName: \"kubernetes.io/host-path/7fd44e8d11c3e0ffe6b1825e2a1f2270-ca-certs\") pod \"kube-controller-manager-minikube\" (UID: \"7fd44e8d11c3e0ffe6b1825e2a1f2270\") " pod="kube-system/kube-controller-manager-minikube"
Jun 21 19:09:31 minikube kubelet[2386]: I0621 19:09:31.057662    2386 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etc-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/7fd44e8d11c3e0ffe6b1825e2a1f2270-etc-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"7fd44e8d11c3e0ffe6b1825e2a1f2270\") " pod="kube-system/kube-controller-manager-minikube"
Jun 21 19:09:31 minikube kubelet[2386]: I0621 19:09:31.057677    2386 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"k8s-certs\" (UniqueName: \"kubernetes.io/host-path/7fd44e8d11c3e0ffe6b1825e2a1f2270-k8s-certs\") pod \"kube-controller-manager-minikube\" (UID: \"7fd44e8d11c3e0ffe6b1825e2a1f2270\") " pod="kube-system/kube-controller-manager-minikube"
Jun 21 19:09:31 minikube kubelet[2386]: I0621 19:09:31.057697    2386 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kubeconfig\" (UniqueName: \"kubernetes.io/host-path/7fd44e8d11c3e0ffe6b1825e2a1f2270-kubeconfig\") pod \"kube-controller-manager-minikube\" (UID: \"7fd44e8d11c3e0ffe6b1825e2a1f2270\") " pod="kube-system/kube-controller-manager-minikube"
Jun 21 19:09:31 minikube kubelet[2386]: I0621 19:09:31.057718    2386 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-local-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/3c555f828409b009ebee39fdbedfcac0-usr-local-share-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"3c555f828409b009ebee39fdbedfcac0\") " pod="kube-system/kube-apiserver-minikube"
Jun 21 19:09:31 minikube kubelet[2386]: I0621 19:09:31.057742    2386 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"flexvolume-dir\" (UniqueName: \"kubernetes.io/host-path/7fd44e8d11c3e0ffe6b1825e2a1f2270-flexvolume-dir\") pod \"kube-controller-manager-minikube\" (UID: \"7fd44e8d11c3e0ffe6b1825e2a1f2270\") " pod="kube-system/kube-controller-manager-minikube"
Jun 21 19:09:31 minikube kubelet[2386]: I0621 19:09:31.057799    2386 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-local-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/7fd44e8d11c3e0ffe6b1825e2a1f2270-usr-local-share-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"7fd44e8d11c3e0ffe6b1825e2a1f2270\") " pod="kube-system/kube-controller-manager-minikube"
Jun 21 19:09:31 minikube kubelet[2386]: I0621 19:09:31.057900    2386 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kubeconfig\" (UniqueName: \"kubernetes.io/host-path/f9c8e1d0d74b1727abdb4b4a31d3a7c1-kubeconfig\") pod \"kube-scheduler-minikube\" (UID: \"f9c8e1d0d74b1727abdb4b4a31d3a7c1\") " pod="kube-system/kube-scheduler-minikube"
Jun 21 19:09:31 minikube kubelet[2386]: I0621 19:09:31.598284    2386 apiserver.go:52] "Watching apiserver"
Jun 21 19:09:31 minikube kubelet[2386]: I0621 19:09:31.606333    2386 desired_state_of_world_populator.go:157] "Finished populating initial desired state of world"
Jun 21 19:09:31 minikube kubelet[2386]: E0621 19:09:31.873393    2386 kubelet.go:1928] "Failed creating a mirror pod for" err="pods \"kube-apiserver-minikube\" already exists" pod="kube-system/kube-apiserver-minikube"
Jun 21 19:09:31 minikube kubelet[2386]: I0621 19:09:31.972075    2386 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/kube-scheduler-minikube" podStartSLOduration=1.972051144 podStartE2EDuration="1.972051144s" podCreationTimestamp="2024-06-21 19:09:30 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2024-06-21 19:09:31.887431395 +0000 UTC m=+1.382724455" watchObservedRunningTime="2024-06-21 19:09:31.972051144 +0000 UTC m=+1.467344204"
Jun 21 19:09:31 minikube kubelet[2386]: I0621 19:09:31.983215    2386 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/etcd-minikube" podStartSLOduration=1.983194298 podStartE2EDuration="1.983194298s" podCreationTimestamp="2024-06-21 19:09:30 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2024-06-21 19:09:31.972270853 +0000 UTC m=+1.467563813" watchObservedRunningTime="2024-06-21 19:09:31.983194298 +0000 UTC m=+1.478487258"
Jun 21 19:09:32 minikube kubelet[2386]: I0621 19:09:32.061567    2386 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/kube-controller-manager-minikube" podStartSLOduration=2.06151299 podStartE2EDuration="2.06151299s" podCreationTimestamp="2024-06-21 19:09:30 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2024-06-21 19:09:31.983412407 +0000 UTC m=+1.478705467" watchObservedRunningTime="2024-06-21 19:09:32.06151299 +0000 UTC m=+1.556806050"
Jun 21 19:09:35 minikube kubelet[2386]: I0621 19:09:35.778478    2386 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/kube-apiserver-minikube" podStartSLOduration=5.778455777 podStartE2EDuration="5.778455777s" podCreationTimestamp="2024-06-21 19:09:30 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2024-06-21 19:09:32.062062812 +0000 UTC m=+1.557355772" watchObservedRunningTime="2024-06-21 19:09:35.778455777 +0000 UTC m=+5.273748837"
Jun 21 19:09:44 minikube kubelet[2386]: I0621 19:09:44.723504    2386 topology_manager.go:215] "Topology Admit Handler" podUID="fb455d40-05d4-4fe4-b7be-54a5edeea3dc" podNamespace="kube-system" podName="storage-provisioner"
Jun 21 19:09:44 minikube kubelet[2386]: I0621 19:09:44.772619    2386 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-fklbn\" (UniqueName: \"kubernetes.io/projected/fb455d40-05d4-4fe4-b7be-54a5edeea3dc-kube-api-access-fklbn\") pod \"storage-provisioner\" (UID: \"fb455d40-05d4-4fe4-b7be-54a5edeea3dc\") " pod="kube-system/storage-provisioner"
Jun 21 19:09:44 minikube kubelet[2386]: I0621 19:09:44.772680    2386 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/host-path/fb455d40-05d4-4fe4-b7be-54a5edeea3dc-tmp\") pod \"storage-provisioner\" (UID: \"fb455d40-05d4-4fe4-b7be-54a5edeea3dc\") " pod="kube-system/storage-provisioner"
Jun 21 19:09:45 minikube kubelet[2386]: I0621 19:09:45.357103    2386 topology_manager.go:215] "Topology Admit Handler" podUID="b5d7ff18-1d93-4491-ba12-0b343ba43c79" podNamespace="kube-system" podName="kube-proxy-pz48x"
Jun 21 19:09:45 minikube kubelet[2386]: I0621 19:09:45.376058    2386 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/b5d7ff18-1d93-4491-ba12-0b343ba43c79-lib-modules\") pod \"kube-proxy-pz48x\" (UID: \"b5d7ff18-1d93-4491-ba12-0b343ba43c79\") " pod="kube-system/kube-proxy-pz48x"
Jun 21 19:09:45 minikube kubelet[2386]: I0621 19:09:45.376138    2386 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-proxy\" (UniqueName: \"kubernetes.io/configmap/b5d7ff18-1d93-4491-ba12-0b343ba43c79-kube-proxy\") pod \"kube-proxy-pz48x\" (UID: \"b5d7ff18-1d93-4491-ba12-0b343ba43c79\") " pod="kube-system/kube-proxy-pz48x"
Jun 21 19:09:45 minikube kubelet[2386]: I0621 19:09:45.376164    2386 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/b5d7ff18-1d93-4491-ba12-0b343ba43c79-xtables-lock\") pod \"kube-proxy-pz48x\" (UID: \"b5d7ff18-1d93-4491-ba12-0b343ba43c79\") " pod="kube-system/kube-proxy-pz48x"
Jun 21 19:09:45 minikube kubelet[2386]: I0621 19:09:45.376215    2386 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-ftbr5\" (UniqueName: \"kubernetes.io/projected/b5d7ff18-1d93-4491-ba12-0b343ba43c79-kube-api-access-ftbr5\") pod \"kube-proxy-pz48x\" (UID: \"b5d7ff18-1d93-4491-ba12-0b343ba43c79\") " pod="kube-system/kube-proxy-pz48x"
Jun 21 19:09:45 minikube kubelet[2386]: I0621 19:09:45.418373    2386 topology_manager.go:215] "Topology Admit Handler" podUID="0a07d2e8-e6b4-4d4b-9682-8153cc0bb8ce" podNamespace="kube-system" podName="coredns-7db6d8ff4d-bmgcs"
Jun 21 19:09:45 minikube kubelet[2386]: I0621 19:09:45.477601    2386 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-vvkxv\" (UniqueName: \"kubernetes.io/projected/0a07d2e8-e6b4-4d4b-9682-8153cc0bb8ce-kube-api-access-vvkxv\") pod \"coredns-7db6d8ff4d-bmgcs\" (UID: \"0a07d2e8-e6b4-4d4b-9682-8153cc0bb8ce\") " pod="kube-system/coredns-7db6d8ff4d-bmgcs"
Jun 21 19:09:45 minikube kubelet[2386]: I0621 19:09:45.477717    2386 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/0a07d2e8-e6b4-4d4b-9682-8153cc0bb8ce-config-volume\") pod \"coredns-7db6d8ff4d-bmgcs\" (UID: \"0a07d2e8-e6b4-4d4b-9682-8153cc0bb8ce\") " pod="kube-system/coredns-7db6d8ff4d-bmgcs"
Jun 21 19:09:45 minikube kubelet[2386]: I0621 19:09:45.992976    2386 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/storage-provisioner" podStartSLOduration=13.992954532 podStartE2EDuration="13.992954532s" podCreationTimestamp="2024-06-21 19:09:32 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2024-06-21 19:09:45.992740723 +0000 UTC m=+15.488033783" watchObservedRunningTime="2024-06-21 19:09:45.992954532 +0000 UTC m=+15.488247592"
Jun 21 19:09:46 minikube kubelet[2386]: I0621 19:09:46.084043    2386 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="067539294b92033b80e87b7873d4e01579a6d6c20301ed52bee50b808aba81c6"
Jun 21 19:09:47 minikube kubelet[2386]: I0621 19:09:47.113191    2386 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/coredns-7db6d8ff4d-bmgcs" podStartSLOduration=2.113170082 podStartE2EDuration="2.113170082s" podCreationTimestamp="2024-06-21 19:09:45 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2024-06-21 19:09:47.112477454 +0000 UTC m=+16.607770514" watchObservedRunningTime="2024-06-21 19:09:47.113170082 +0000 UTC m=+16.608463142"
Jun 21 19:09:47 minikube kubelet[2386]: I0621 19:09:47.126271    2386 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/kube-proxy-pz48x" podStartSLOduration=2.126249318 podStartE2EDuration="2.126249318s" podCreationTimestamp="2024-06-21 19:09:45 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2024-06-21 19:09:47.126134813 +0000 UTC m=+16.621427773" watchObservedRunningTime="2024-06-21 19:09:47.126249318 +0000 UTC m=+16.621542278"
Jun 21 19:09:51 minikube kubelet[2386]: I0621 19:09:51.348365    2386 kuberuntime_manager.go:1523] "Updating runtime config through cri with podcidr" CIDR="10.244.0.0/24"
Jun 21 19:09:51 minikube kubelet[2386]: I0621 19:09:51.350891    2386 kubelet_network.go:61] "Updating Pod CIDR" originalPodCIDR="" newPodCIDR="10.244.0.0/24"
Jun 21 19:10:07 minikube kubelet[2386]: I0621 19:10:07.278827    2386 scope.go:117] "RemoveContainer" containerID="733f56888f32db55fbff41185c424382fc30d789b00c19aa6ed8de5d82a023e9"
Jun 21 19:10:30 minikube kubelet[2386]: I0621 19:10:30.516183    2386 topology_manager.go:215] "Topology Admit Handler" podUID="3a24761d-2f10-4660-8d8a-ec3589ac205e" podNamespace="default" podName="go-rest-api-f97dd8f58-9zql6"
Jun 21 19:10:30 minikube kubelet[2386]: I0621 19:10:30.623797    2386 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-z8dzt\" (UniqueName: \"kubernetes.io/projected/3a24761d-2f10-4660-8d8a-ec3589ac205e-kube-api-access-z8dzt\") pod \"go-rest-api-f97dd8f58-9zql6\" (UID: \"3a24761d-2f10-4660-8d8a-ec3589ac205e\") " pod="default/go-rest-api-f97dd8f58-9zql6"
Jun 21 19:10:30 minikube kubelet[2386]: I0621 19:10:30.623869    2386 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"host-pv\" (UniqueName: \"kubernetes.io/host-path/3a24761d-2f10-4660-8d8a-ec3589ac205e-host-pv\") pod \"go-rest-api-f97dd8f58-9zql6\" (UID: \"3a24761d-2f10-4660-8d8a-ec3589ac205e\") " pod="default/go-rest-api-f97dd8f58-9zql6"


==> storage-provisioner [707db7b96adf] <==
I0621 19:10:07.412935       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0621 19:10:07.421481       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0621 19:10:07.421529       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0621 19:10:07.430466       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0621 19:10:07.430602       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"f23ffe25-146d-44eb-81f8-79d9bf35dc61", APIVersion:"v1", ResourceVersion:"422", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_189c70c9-a934-4c32-b9a4-c8193512dd84 became leader
I0621 19:10:07.430638       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_189c70c9-a934-4c32-b9a4-c8193512dd84!
I0621 19:10:07.531651       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_189c70c9-a934-4c32-b9a4-c8193512dd84!


==> storage-provisioner [733f56888f32] <==
I0621 19:09:45.374640       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0621 19:10:06.448099       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: connect: connection refused

